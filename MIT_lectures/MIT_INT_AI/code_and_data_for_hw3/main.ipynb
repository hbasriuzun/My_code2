{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing code_for_hw03\n",
      "Imported tidy_plot, plot_separator, plot_data, plot_nonlin_sep, cv, rv, y, positive, score\n",
      "Datasets: super_simple_separable_through_origin(), super_simple_separable(), xor(), xor_more()\n",
      "Tests for part 2: test_linear_classifier_with_features, mul, make_polynomial_feature_fun, \n",
      "                  test_with_features\n",
      "Also loaded: perceptron, one_hot_internal, test_one_hot\n"
     ]
    }
   ],
   "source": [
    "# Implement perceptron, average perceptron, and pegasos\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import pdb\n",
    "import itertools\n",
    "import operator\n",
    "import functools\n",
    "\n",
    "print(\"Importing code_for_hw03\")\n",
    "\n",
    "######################################################################\n",
    "# Plotting\n",
    "\n",
    "def tidy_plot(xmin, xmax, ymin, ymax, center = False, title = None,\n",
    "                 xlabel = None, ylabel = None):\n",
    "    plt.ion()\n",
    "    plt.figure(facecolor=\"white\")\n",
    "    ax = plt.subplot()\n",
    "    if center:\n",
    "        ax.spines['left'].set_position('zero')\n",
    "        ax.spines['right'].set_color('none')\n",
    "        ax.spines['bottom'].set_position('zero')\n",
    "        ax.spines['top'].set_color('none')\n",
    "        ax.spines['left'].set_smart_bounds(True)\n",
    "        ax.spines['bottom'].set_smart_bounds(True)\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "    else:\n",
    "        ax.spines[\"top\"].set_visible(False)    \n",
    "        ax.spines[\"right\"].set_visible(False)    \n",
    "        ax.get_xaxis().tick_bottom()  \n",
    "        ax.get_yaxis().tick_left()\n",
    "    eps = .05\n",
    "    plt.xlim(xmin-eps, xmax+eps)\n",
    "    plt.ylim(ymin-eps, ymax+eps)\n",
    "    if title: ax.set_title(title)\n",
    "    if xlabel: ax.set_xlabel(xlabel)\n",
    "    if ylabel: ax.set_ylabel(ylabel)\n",
    "    return ax\n",
    "\n",
    "def plot_separator(ax, th, th_0):\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    ymin,ymax = ax.get_ylim()\n",
    "    pts = []\n",
    "    eps = 1.0e-6\n",
    "    # xmin boundary crossing is when xmin th[0] + y th[1] + th_0 = 0\n",
    "    # that is, y = (-th_0 - xmin th[0]) / th[1]\n",
    "    if abs(th[1,0]) > eps:\n",
    "        pts += [np.array([x, (-th_0 - x * th[0,0]) / th[1,0]]) \\\n",
    "                                                        for x in (xmin, xmax)]\n",
    "    if abs(th[0,0]) > 1.0e-6:\n",
    "        pts += [np.array([(-th_0 - y * th[1,0]) / th[0,0], y]) \\\n",
    "                                                         for y in (ymin, ymax)]\n",
    "    in_pts = []\n",
    "    for p in pts:\n",
    "        if (xmin-eps) <= p[0] <= (xmax+eps) and \\\n",
    "           (ymin-eps) <= p[1] <= (ymax+eps):\n",
    "            duplicate = False\n",
    "            for p1 in in_pts:\n",
    "                if np.max(np.abs(p - p1)) < 1.0e-6:\n",
    "                    duplicate = True\n",
    "            if not duplicate:\n",
    "                in_pts.append(p)\n",
    "    if in_pts and len(in_pts) >= 2:\n",
    "        # Plot separator\n",
    "        vpts = np.vstack(in_pts)\n",
    "        ax.plot(vpts[:,0], vpts[:,1], 'k-', lw=2)\n",
    "        # Plot normal\n",
    "        vmid = 0.5*(in_pts[0] + in_pts[1])\n",
    "        scale = np.sum(th*th)**0.5\n",
    "        diff = in_pts[0] - in_pts[1]\n",
    "        dist = max(xmax-xmin, ymax-ymin)\n",
    "        vnrm = vmid + (dist/10)*(th.T[0]/scale)\n",
    "        vpts = np.vstack([vmid, vnrm])\n",
    "        ax.plot(vpts[:,0], vpts[:,1], 'k-', lw=2)\n",
    "        # Try to keep limits from moving around\n",
    "        ax.set_xlim((xmin, xmax))\n",
    "        ax.set_ylim((ymin, ymax))\n",
    "    else:\n",
    "        print('Separator not in plot range')\n",
    "\n",
    "def plot_data(data, labels, ax = None, clear = False,\n",
    "                  xmin = None, xmax = None, ymin = None, ymax = None):\n",
    "    if ax is None:\n",
    "        if xmin == None: xmin = np.min(data[0, :]) - 0.5\n",
    "        if xmax == None: xmax = np.max(data[0, :]) + 0.5\n",
    "        if ymin == None: ymin = np.min(data[1, :]) - 0.5\n",
    "        if ymax == None: ymax = np.max(data[1, :]) + 0.5\n",
    "        ax = tidy_plot(xmin, xmax, ymin, ymax)\n",
    "\n",
    "        x_range = xmax - xmin; y_range = ymax - ymin\n",
    "        if .1 < x_range / y_range < 10:\n",
    "            ax.set_aspect('equal')\n",
    "        xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "    elif clear:\n",
    "        xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "        ax.clear()\n",
    "    else:\n",
    "        xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "    colors = np.choose(labels > 0, cv(['r', 'g']))[0]\n",
    "    ax.scatter(data[0,:], data[1,:], c = colors,\n",
    "                    marker = 'o', s=50, edgecolors = 'none')\n",
    "    # Seems to occasionally mess up the limits\n",
    "    ax.set_xlim(xlim); ax.set_ylim(ylim)\n",
    "    ax.grid(True, which='both')\n",
    "    #ax.axhline(y=0, color='k')\n",
    "    #ax.axvline(x=0, color='k')\n",
    "    return ax\n",
    "\n",
    "# Must either specify limits or existing ax\n",
    "def plot_nonlin_sep(predictor, ax = None, xmin = None , xmax = None,\n",
    "                        ymin = None, ymax = None, res = 30):\n",
    "    if ax is None:\n",
    "        ax = tidy_plot(xmin, xmax, ymin, ymax)\n",
    "    else:\n",
    "        if xmin == None:\n",
    "            xmin, xmax = ax.get_xlim()\n",
    "            ymin, ymax = ax.get_ylim()\n",
    "        else:\n",
    "            ax.set_xlim((xmin, xmax))\n",
    "            ax.set_ylim((ymin, ymax))\n",
    "\n",
    "    cmap = colors.ListedColormap(['black', 'white'])\n",
    "    bounds=[-2,0,2]\n",
    "    norm = colors.BoundaryNorm(bounds, cmap.N)            \n",
    "            \n",
    "    ima = np.array([[predictor(x1i, x2i) \\\n",
    "                         for x1i in np.linspace(xmin, xmax, res)] \\\n",
    "                         for x2i in np.linspace(ymin, ymax, res)])\n",
    "    im = ax.imshow(np.flipud(ima), interpolation = 'none',\n",
    "                       extent = [xmin, xmax, ymin, ymax],\n",
    "                       cmap = cmap, norm = norm)\n",
    "\n",
    "######################################################################\n",
    "#   Utilities\n",
    "\n",
    "# Takes a list of numbers and returns a column vector:  n x 1\n",
    "def cv(value_list):\n",
    "    return np.transpose(rv(value_list))\n",
    "\n",
    "# Takes a list of numbers and returns a row vector: 1 x n\n",
    "def rv(value_list):\n",
    "    return np.array([value_list])\n",
    "\n",
    "# x is dimension d by 1\n",
    "# th is dimension d by 1\n",
    "# th0 is a scalar\n",
    "# return a 1 by 1 matrix\n",
    "def y(x, th, th0):\n",
    "   return np.dot(np.transpose(th), x) + th0\n",
    "\n",
    "# x is dimension d by 1\n",
    "# th is dimension d by 1\n",
    "# th0 is dimension 1 by 1\n",
    "# return 1 by 1 matrix of +1, 0, -1\n",
    "def positive(x, th, th0):\n",
    "   return np.sign(y(x, th, th0))\n",
    "\n",
    "# data is dimension d by n\n",
    "# labels is dimension 1 by n\n",
    "# ths is dimension d by 1\n",
    "# th0s is dimension 1 by 1\n",
    "# return 1 by 1 matrix of integer indicating number of data points correct for\n",
    "# each separator.\n",
    "def score(data, labels, th, th0):\n",
    "   return np.sum(positive(data, th, th0) == labels)\n",
    "\n",
    "######################################################################\n",
    "#   Data Sets\n",
    "\n",
    "# Return d = 2 by n = 4 data matrix and 1 x n = 4 label matrix\n",
    "def super_simple_separable_through_origin():\n",
    "    X = np.array([[2, 3, 9, 12],\n",
    "                  [5, 1, 6, 5]])\n",
    "    y = np.array([[1, -1, 1, -1]])\n",
    "    return X, y\n",
    "\n",
    "def super_simple_separable():\n",
    "    X = np.array([[2, 3, 9, 12],\n",
    "                  [5, 2, 6, 5]])\n",
    "    y = np.array([[1, -1, 1, -1]])\n",
    "    return X, y\n",
    "\n",
    "def xor():\n",
    "    X = np.array([[1, 2, 1, 2],\n",
    "                  [1, 2, 2, 1]])\n",
    "    y = np.array([[1, 1, -1, -1]])\n",
    "    return X, y\n",
    "\n",
    "def xor_more():\n",
    "    X = np.array([[1, 2, 1, 2, 2, 4, 1, 3],\n",
    "                  [1, 2, 2, 1, 3, 1, 3, 3]])\n",
    "    y = np.array([[1, 1, -1, -1, 1, 1, -1, -1]])\n",
    "    return X, y\n",
    "\n",
    "######################################################################\n",
    "#   Tests for part 2:  features\n",
    "\n",
    "# Make it take miscellaneous args and pass into learner\n",
    "def test_linear_classifier_with_features(dataFun, learner, feature_fun,\n",
    "                             draw = True, refresh = True, pause = True):\n",
    "    raw_data, labels = dataFun()\n",
    "    data = feature_fun(raw_data) if feature_fun else raw_data\n",
    "    if draw:\n",
    "        ax = plot_data(raw_data, labels)\n",
    "        def hook(params):\n",
    "            (th, th0) = params\n",
    "            plot_nonlin_sep(\n",
    "                lambda x1,x2: int(positive(feature_fun(cv([x1, x2])), th, th0)),\n",
    "                ax = ax)\n",
    "            plot_data(raw_data, labels, ax)\n",
    "            plt.pause(0.05)\n",
    "            print('th', th.T, 'th0', th0)\n",
    "            if pause: input('press enter here to continue:')\n",
    "    else:\n",
    "        hook = None\n",
    "    th, th0 = learner(data, labels, hook = hook)\n",
    "    if hook: hook((th, th0))\n",
    "    print(\"Final score\", int(score(data, labels, th, th0)))\n",
    "    print(\"Params\", np.transpose(th), th0)\n",
    "\n",
    "def mul(seq):\n",
    "    return functools.reduce(operator.mul, seq, 1)\n",
    "\n",
    "def make_polynomial_feature_fun(order):\n",
    "    # raw_features is d by n\n",
    "    # return is k by n where k = sum_{i = 0}^order  multichoose(d, i)\n",
    "    def f(raw_features):\n",
    "        d, n = raw_features.shape\n",
    "        result = []   # list of column vectors\n",
    "        for j in range(n):\n",
    "            features = []\n",
    "            for o in range(order+1):\n",
    "                indexTuples = \\\n",
    "                          itertools.combinations_with_replacement(range(d), o)\n",
    "                for it in indexTuples:\n",
    "                    features.append(mul(raw_features[i, j] for i in it))\n",
    "            result.append(cv(features))\n",
    "        return np.hstack(result)\n",
    "    return f\n",
    "\n",
    "def test_with_features(dataFun, order = 2, draw=True, pause=True):\n",
    "    test_linear_classifier_with_features(\n",
    "        dataFun,                        # data\n",
    "        perceptron,                     # learner\n",
    "        make_polynomial_feature_fun(order), # feature maker\n",
    "        draw=draw,\n",
    "        pause=pause)\n",
    "\n",
    "# Perceptron algorithm with offset.\n",
    "# data is dimension d by n\n",
    "# labels is dimension 1 by n\n",
    "# T is a positive integer number of steps to run\n",
    "def perceptron(data, labels, params = {}, hook = None):\n",
    "    T = params.get('T', 100)\n",
    "    (d, n) = data.shape\n",
    "    m = 0\n",
    "    theta = np.zeros((d, 1)); theta_0 = np.zeros((1, 1))\n",
    "    for t in range(T):\n",
    "        for i in range(n):\n",
    "            x = data[:,i:i+1]\n",
    "            y = labels[:,i:i+1]\n",
    "            if y * positive(x, theta, theta_0) <= 0.0:\n",
    "                m += 1\n",
    "                theta = theta + y * x\n",
    "                theta_0 = theta_0 + y\n",
    "                if hook: hook((theta, theta_0))\n",
    "    return theta, theta_0, m\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "#   Tests for part 2D:  Encoding discrete values\n",
    "\n",
    "def one_hot_internal(x, k):\n",
    "    # Make an empty column vector\n",
    "    v = np.zeros((k, 1))\n",
    "    # Set an entry to 1\n",
    "    v[x-1, 0] = 1\n",
    "    return v\n",
    "\n",
    "def test_one_hot(sub):\n",
    "    if one_hot_internal(3, 5).tolist() == sub(3, 5).tolist() and one_hot_internal(4, 7).tolist() == sub(4, 7).tolist():\n",
    "        print(\"Passed! \\n\")\n",
    "    else: print(\"Test Failed\")\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "print(\"Imported tidy_plot, plot_separator, plot_data, plot_nonlin_sep, cv, rv, y, positive, score\")\n",
    "print(\"Datasets: super_simple_separable_through_origin(), super_simple_separable(), xor(), xor_more()\")\n",
    "print(\"Tests for part 2: test_linear_classifier_with_features, mul, make_polynomial_feature_fun, \")\n",
    "print(\"                  test_with_features\")\n",
    "print(\"Also loaded: perceptron, one_hot_internal, test_one_hot\")\n",
    "\n",
    "######################################################################\n",
    "#   Example for part 3B) test_with_features()\n",
    "#test_with_features(super_simple_separable, 2, draw=True, pause=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Margin calcuator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26832816]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Veri seti\n",
    "data = np.array([[200, 800, 200, 800],\n",
    "                [0.2,  0.2,  0.8,  0.8],\n",
    "                [1,1,1,1]])\n",
    "\n",
    "# Etiketler\n",
    "labels = np.array([[-1, -1, 1, 1]])\n",
    "\n",
    "# Sınıflandırıcı parametreleri\n",
    "w = np.array([0, 1, -0.5])\n",
    "\n",
    "# Marjin hesaplama fonksiyonu\n",
    "def calculate_margin(w, x, y):\n",
    "    return y * np.dot(w, x) / np.linalg.norm(w)\n",
    "\n",
    "(d, n) = data.shape\n",
    "\n",
    "# Her veri noktası için marjini hesapla\n",
    "margins = [calculate_margin(w, data[:,i:i+1], labels[:,i:i+1]) for i in range(d)]\n",
    "\n",
    "# En küçük marjini bul\n",
    "min_margin = min(margins)\n",
    "print(min_margin.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teorik Hata Sınırı: [[8888903.33333334]]\n",
      "[[8888903.33333334]]\n"
     ]
    }
   ],
   "source": [
    "# Veri noktalarının maksimum normunu hesapla\n",
    "max_norm = max(np.linalg.norm(data[:,i:i+1]) for i in range(d))\n",
    "\n",
    "# Teorik hata sınırını hesapla\n",
    "theoretical_bound = (max_norm ** 2) / (min_margin ** 2)\n",
    "\n",
    "# Sonucu yazdır\n",
    "print(f\"Teorik Hata Sınırı: {theoretical_bound}\")\n",
    "print(((max(np.linalg.norm(data[:,i:i+1]) for i in range(d))) ** 2) / (min_margin ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ağırlıklar (theta): \n",
      "[[  0.]\n",
      " [600.]\n",
      " [  0.]]\n",
      "Bias (theta_0): \n",
      "[[0]]\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "def perceptron(data, labels, params = {}, hook = None):\n",
    "    T = params.get('T', 100)\n",
    "    (d, n) = data.shape\n",
    "    m = 0\n",
    "    theta = np.array([[0],[0],[0]]); theta_0 = np.array([[0]])\n",
    "    for t in range(T):\n",
    "        for i in range(n):\n",
    "            x = data[:,i:i+1]\n",
    "            y = labels[:,i:i+1]\n",
    "            if y * positive(x, theta, theta_0) <= 0.0:\n",
    "                m += 1\n",
    "                theta = theta + y * x\n",
    "                #theta_0 = theta_0 + y\n",
    "                if hook: hook((theta, theta_0))\n",
    "    return theta, theta_0, m\n",
    "\n",
    "data = np.array([[200, 800, 200, 800],\n",
    "                [0.2,  0.2,  0.8,  0.8],\n",
    "                [1,1,1,1]])\n",
    "\n",
    "# Etiketler\n",
    "labels = np.array([[-1, -1, 1, 1]])\n",
    "params = {'T': 1000}  # 100 iterasyon için parametre\n",
    "\n",
    "theta, theta_0, m = perceptron(data, labels,params=params)\n",
    "\n",
    "print(f\"Ağırlıklar (theta): \\n{theta}\")\n",
    "print(f\"Bias (theta_0): \\n{theta_0}\")\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26832816]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Veri seti\n",
    "data = np.array([[200, 800, 200, 800],\n",
    "                [0.2,  0.2,  0.8,  0.8],\n",
    "                [1,1,1,1]])\n",
    "data[0] = data[0] * 0.001\n",
    "\n",
    "# Etiketler\n",
    "labels = np.array([[-1, -1, 1, 1]])\n",
    "\n",
    "# Sınıflandırıcı parametreleri\n",
    "w = np.array([0, 1, -0.5])\n",
    "\n",
    "# Marjin hesaplama fonksiyonu\n",
    "def calculate_margin(w, x, y):\n",
    "    return y * np.dot(w, x) / np.linalg.norm(w)\n",
    "\n",
    "(d, n) = data.shape\n",
    "\n",
    "# Her veri noktası için marjini hesapla\n",
    "margins = [calculate_margin(w, data[:,i:i+1], labels[:,i:i+1]) for i in range(d)]\n",
    "\n",
    "# En küçük marjini bul\n",
    "min_margin = min(margins)\n",
    "print(min_margin.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ağırlıklar (theta): \n",
      "[[-0.2]\n",
      " [ 1.6]\n",
      " [-1. ]]\n",
      "Bias (theta_0): \n",
      "[[0]]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "def perceptron(data, labels, params = {}, hook = None):\n",
    "    T = params.get('T', 100)\n",
    "    (d, n) = data.shape\n",
    "    m = 0\n",
    "    theta = np.array([[0],[0],[0]]); theta_0 = np.array([[0]])\n",
    "    for t in range(T):\n",
    "        for i in range(n):\n",
    "            x = data[:,i:i+1]\n",
    "            y = labels[:,i:i+1]\n",
    "            if y * positive(x, theta, theta_0) <= 0.0:\n",
    "                m += 1\n",
    "                theta = theta + y * x\n",
    "                #theta_0 = theta_0 + y\n",
    "                if hook: hook((theta, theta_0))\n",
    "    return theta, theta_0, m\n",
    "\n",
    "data = np.array([[200, 800, 200, 800],\n",
    "                [0.2,  0.2,  0.8,  0.8],\n",
    "                [1,1,1,1]])\n",
    "data[0] = data[0] * 0.001\n",
    "\n",
    "# Etiketler\n",
    "labels = np.array([[-1, -1, 1, 1]])\n",
    "params = {'T': 10000}  # 100 iterasyon için parametre\n",
    "\n",
    "theta, theta_0, m = perceptron(data, labels,params=params)\n",
    "\n",
    "print(f\"Ağırlıklar (theta): \\n{theta}\")\n",
    "print(f\"Bias (theta_0): \\n{theta_0}\")\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ağırlıklar (theta): \n",
      "[[2]]\n",
      "Bias (theta_0): \n",
      "[[-7]]\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "def perceptron(data, labels, params = {}, hook = None):\n",
    "    T = params.get('T', 100)\n",
    "    (d, n) = data.shape\n",
    "    m = 0\n",
    "    theta = np.array([[0]]); theta_0 = np.array([[0]])\n",
    "    for t in range(T):\n",
    "        for i in range(n):\n",
    "            x = data[:,i:i+1]\n",
    "            y = labels[:,i:i+1]\n",
    "            if y * positive(x, theta, theta_0) <= 0.0:\n",
    "                m += 1\n",
    "                theta = theta + y * x\n",
    "                theta_0 = theta_0 + y\n",
    "                if hook: hook((theta, theta_0))\n",
    "    return theta, theta_0, m\n",
    "\n",
    "data = np.array([[2, 3, 4, 5]])\n",
    "labels = np.array([[1, 1, -1, -1]])\n",
    "\n",
    "# Etiketler\n",
    "labels = np.array([[-1, -1, 1, 1]])\n",
    "params = {'T': 10000}  # 100 iterasyon için parametre\n",
    "\n",
    "theta, theta_0, m = perceptron(data, labels,params=params)\n",
    "\n",
    "print(f\"Ağırlıklar (theta): \\n{theta}\")\n",
    "print(f\"Bias (theta_0): \\n{theta_0}\")\n",
    "print(m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, k):\n",
    "    d = np.zeros((k,1))\n",
    "    d[x-1]=1;\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `one_hot` Fonksiyonu Açıklaması\n",
    "\n",
    "Bu fonksiyon, verilen bir sınıf etiketini `one-hot` kodlamasına dönüştürür. `one-hot` kodlama, bir kategorik değişkeni binary (ikili) vektör formatında temsil etmenin yaygın bir yoludur. Her vektörde yalnızca bir eleman `1` değerini alır, diğer tüm elemanlar `0` olur. Bu, sınıf etiketinin konumunu belirtir.\n",
    "\n",
    "### Fonksiyon Tanımı\n",
    "\n",
    "```python\n",
    "def one_hot(x, k):\n",
    "    d = np.zeros((k, 1))\n",
    "    d[x-1] = 1\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]]\n",
      "Ağırlıklar (theta): \n",
      "[[ 0.]\n",
      " [ 2.]\n",
      " [ 1.]\n",
      " [-2.]\n",
      " [-1.]\n",
      " [ 0.]]\n",
      "Bias (theta_0): \n",
      "[[0.]]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = np.array([[2, 3,  4,  5]])\n",
    "labels = np.array([[1, 1, -1, -1]])\n",
    "\n",
    "(d, n) = data.shape\n",
    "\n",
    "hot_data = np.hstack([one_hot_internal(data[0, i], 6) for i in range(n)])\n",
    "\n",
    "print(hot_data)\n",
    "# Perceptron fonksiyonunun tanımlanması\n",
    "def perceptron(data, labels, params = {}, hook = None):\n",
    "    T = params.get('T', 100)\n",
    "    (d, n) = data.shape\n",
    "    m = 0\n",
    "    theta = np.zeros((d, 1)); theta_0 = np.zeros((1, 1))\n",
    "    for t in range(T):\n",
    "        for i in range(n):\n",
    "            x = data[:,i:i+1]\n",
    "            y = labels[:,i:i+1]\n",
    "            if y * positive(x, theta, theta_0) <= 0.0:\n",
    "                m += 1\n",
    "                theta = theta + y * x\n",
    "                theta_0 = theta_0 + y\n",
    "                if hook: hook((theta, theta_0))\n",
    "    return theta, theta_0, m\n",
    "\n",
    "\n",
    "# Etiketler\n",
    "params = {'T': 1000}  # 100 iterasyon için parametre\n",
    "\n",
    "theta, theta_0,m = perceptron(hot_data, labels,params=params)\n",
    "\n",
    "print(f\"Ağırlıklar (theta): \\n{theta}\")\n",
    "print(f\"Bias (theta_0): \\n{theta_0}\")\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding veri seti oluşturma fonksiyonu\n",
    "def one_hot_encode_data(data, num_classes):\n",
    "    n = data.shape[1]\n",
    "    return np.hstack([one_hot_internal(data[0, i], num_classes) for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samsung için tahmin: -1\n",
      "Nokia için tahmin: -1\n",
      "[-1, -1]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 6\n",
    "# Samsung ve Nokia için one-hot kodlanmış vektörler\n",
    "# Samsung = 1, Nokia = 6\n",
    "samsung_one_hot = one_hot_internal(1, num_classes)\n",
    "nokia_one_hot = one_hot_internal(6, num_classes)\n",
    "\n",
    "# Tahminleri yapma\n",
    "def predict(x, theta, theta_0):\n",
    "    return 1 if positive(x, theta, theta_0) > 0 else -1\n",
    "\n",
    "samsung_prediction = predict(samsung_one_hot, theta, theta_0)\n",
    "nokia_prediction = predict(nokia_one_hot, theta, theta_0)\n",
    "\n",
    "# Tahminleri yazdırma\n",
    "print(f\"Samsung için tahmin: {samsung_prediction}\")\n",
    "print(f\"Nokia için tahmin: {nokia_prediction}\")\n",
    "\n",
    "# Sonuçları bir liste olarak döndürme\n",
    "predictions = [samsung_prediction, nokia_prediction]\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samsung için mesafe: [[0.]]\n",
      "Nokia için mesafe: [[0.]]\n"
     ]
    }
   ],
   "source": [
    "# Mesafe hesaplama fonksiyonu\n",
    "def distance_from_separator(x, theta, theta_0):\n",
    "    return np.abs(positive(x, theta, theta_0)) / np.linalg.norm(theta)\n",
    "\n",
    "# Mesafeleri hesaplama\n",
    "samsung_distance = distance_from_separator(samsung_one_hot, theta, theta_0)\n",
    "nokia_distance = distance_from_separator(nokia_one_hot, theta, theta_0)\n",
    "\n",
    "# Mesafeleri yazdırma\n",
    "print(f\"Samsung için mesafe: {samsung_distance}\")\n",
    "print(f\"Nokia için mesafe: {nokia_distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "Ağırlıklar (theta): \n",
      "[[ 1.]\n",
      " [ 1.]\n",
      " [-2.]\n",
      " [-2.]\n",
      " [ 1.]\n",
      " [ 1.]]\n",
      "Bias (theta_0): \n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "data =   np.array([[1, 2, 3, 4, 5, 6]])\n",
    "labels = np.array([[1, 1, -1, -1, 1, 1]])\n",
    "\n",
    "n = data.shape[1]\n",
    "hot_data = np.hstack([one_hot_internal(data[0, i], 6) for i in range(n)])\n",
    "print(hot_data)\n",
    "\n",
    "theta, theta_0,m = perceptron(hot_data, labels)\n",
    "print(f\"Ağırlıklar (theta): \\n{theta}\")\n",
    "print(f\"Bias (theta_0): \\n{theta_0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5]\n",
      " [2]]\n",
      "(3, 1)\n",
      "(66, 1)\n",
      "(231, 1)\n",
      "(496, 1)\n",
      "(861, 1)\n",
      "(1326, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kc/txgh3xkn0xgfsxb3fpxhwsvc0000gn/T/ipykernel_84269/808165638.py:224: RuntimeWarning: overflow encountered in long_scalars\n",
      "  return functools.reduce(operator.mul, seq, 1)\n"
     ]
    }
   ],
   "source": [
    "data =  np.array([1, 10, 20, 30, 40, 50])\n",
    "data2 =  np.array([[5],\n",
    "                   [2]])\n",
    "print(data2)\n",
    "\n",
    "for i in data:\n",
    "    pol_data = make_polynomial_feature_fun(i)\n",
    "    print(pol_data(data2).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]] th0 [[1.]]\n",
      "th [[ 0.  0. -1.  0. -1. -3.  0. -1. -3. -7.]] th0 [[0.]]\n",
      "th [[ 1.  2.  2.  4.  5.  6.  8. 11. 15. 20.]] th0 [[1.]]\n",
      "th [[ 0.  1. -1.  3.  2. -3.  7.  8.  6. -7.]] th0 [[0.]]\n",
      "th [[ -1.  -2.  -4.  -6.  -7. -12. -20. -19. -21. -34.]] th0 [[-1.]]\n",
      "th [[  0.  -1.  -3.  -5.  -6. -11. -19. -18. -20. -33.]] th0 [[0.]]\n",
      "th [[  1.   1.  -1.  -1.  -2.  -7. -11. -10. -12. -25.]] th0 [[1.]]\n",
      "th [[ 2.  3.  2.  3.  4.  2. -3.  2.  6.  2.]] th0 [[2.]]\n",
      "th [[ 3.  7.  3. 19.  8.  3. 61. 18. 10.  3.]] th0 [[3.]]\n",
      "th [[  2.   6.   0.  18.   5.  -6.  60.  15.   1. -24.]] th0 [[2.]]\n",
      "th [[  1.   3.  -3.   9.  -4. -15.  33. -12. -26. -51.]] th0 [[1.]]\n",
      "th [[  2.   4.  -2.  10.  -3. -14.  34. -11. -25. -50.]] th0 [[2.]]\n",
      "th [[  3.   6.   0.  14.   1. -10.  42.  -3. -17. -42.]] th0 [[3.]]\n",
      "th [[  2.   4.  -1.  10.  -1. -11.  34.  -7. -19. -43.]] th0 [[2.]]\n",
      "th [[  3.   6.   2.  14.   5.  -2.  42.   5.  -1. -16.]] th0 [[3.]]\n",
      "th [[  2.   3.  -1.   5.  -4. -11.  15. -22. -28. -43.]] th0 [[2.]]\n",
      "th [[  3.   4.   0.   6.  -3. -10.  16. -21. -27. -42.]] th0 [[3.]]\n",
      "th [[  4.   6.   2.  10.   1.  -6.  24. -13. -19. -34.]] th0 [[4.]]\n",
      "th [[  3.   4.   1.   6.  -1.  -7.  16. -17. -21. -35.]] th0 [[3.]]\n",
      "th [[ 4.  6.  4. 10.  5.  2. 24. -5. -3. -8.]] th0 [[4.]]\n",
      "th [[  3.   3.   1.   1.  -4.  -7.  -3. -32. -30. -35.]] th0 [[3.]]\n",
      "th [[  4.   4.   2.   2.  -3.  -6.  -2. -31. -29. -34.]] th0 [[4.]]\n",
      "th [[  5.   6.   4.   6.   1.  -2.   6. -23. -21. -26.]] th0 [[5.]]\n",
      "th [[  6.   8.   7.  10.   7.   7.  14. -11.  -3.   1.]] th0 [[6.]]\n",
      "th [[  5.   7.   4.   9.   4.  -2.  13. -14. -12. -26.]] th0 [[5.]]\n",
      "th [[  6.   8.   5.  10.   5.  -1.  14. -13. -11. -25.]] th0 [[6.]]\n",
      "th [[  7.  10.   7.  14.   9.   3.  22.  -5.  -3. -17.]] th0 [[7.]]\n",
      "th [[  6.   8.   6.  10.   7.   2.  14.  -9.  -5. -18.]] th0 [[6.]]\n",
      "th [[ 7. 10.  9. 14. 13. 11. 22.  3. 13.  9.]] th0 [[7.]]\n",
      "th [[  6.   9.   6.  13.  10.   2.  21.   0.   4. -18.]] th0 [[6.]]\n",
      "th [[  5.   6.   3.   4.   1.  -7.  -6. -27. -23. -45.]] th0 [[5.]]\n",
      "th [[  6.   7.   4.   5.   2.  -6.  -5. -26. -22. -44.]] th0 [[6.]]\n",
      "th [[  7.   9.   6.   9.   6.  -2.   3. -18. -14. -36.]] th0 [[7.]]\n",
      "th [[ 8. 11.  9. 13. 12.  7. 11. -6.  4. -9.]] th0 [[8.]]\n",
      "th [[  7.   8.   6.   4.   3.  -2. -16. -33. -23. -36.]] th0 [[7.]]\n",
      "th [[  8.   9.   7.   5.   4.  -1. -15. -32. -22. -35.]] th0 [[8.]]\n",
      "th [[  9.  11.   9.   9.   8.   3.  -7. -24. -14. -27.]] th0 [[9.]]\n",
      "th [[ 10.  13.  12.  13.  14.  12.   1. -12.   4.   0.]] th0 [[10.]]\n",
      "th [[  9.  12.   9.  12.  11.   3.   0. -15.  -5. -27.]] th0 [[9.]]\n",
      "th [[ 10.  14.  11.  16.  15.   7.   8.  -7.   3. -19.]] th0 [[10.]]\n",
      "th [[  9.  12.  10.  12.  13.   6.   0. -11.   1. -20.]] th0 [[9.]]\n",
      "th [[10. 14. 13. 16. 19. 15.  8.  1. 19.  7.]] th0 [[10.]]\n",
      "th [[  9.  13.  10.  15.  16.   6.   7.  -2.  10. -20.]] th0 [[9.]]\n",
      "th [[  8.  10.   7.   6.   7.  -3. -20. -29. -17. -47.]] th0 [[8.]]\n",
      "th [[  9.  11.   8.   7.   8.  -2. -19. -28. -16. -46.]] th0 [[9.]]\n",
      "th [[ 10.  13.  10.  11.  12.   2. -11. -20.  -8. -38.]] th0 [[10.]]\n",
      "th [[ 11.  15.  13.  15.  18.  11.  -3.  -8.  10. -11.]] th0 [[11.]]\n",
      "th [[ 10.  14.  10.  14.  15.   2.  -4. -11.   1. -38.]] th0 [[10.]]\n",
      "th [[ 11.  16.  12.  18.  19.   6.   4.  -3.   9. -30.]] th0 [[11.]]\n",
      "th [[ 10.  14.  11.  14.  17.   5.  -4.  -7.   7. -31.]] th0 [[10.]]\n",
      "th [[11. 16. 14. 18. 23. 14.  4.  5. 25. -4.]] th0 [[11.]]\n",
      "th [[ 10.  15.  11.  17.  20.   5.   3.   2.  16. -31.]] th0 [[10.]]\n",
      "th [[  9.  12.   8.   8.  11.  -4. -24. -25. -11. -58.]] th0 [[9.]]\n",
      "th [[ 10.  13.   9.   9.  12.  -3. -23. -24. -10. -57.]] th0 [[10.]]\n",
      "th [[ 11.  15.  11.  13.  16.   1. -15. -16.  -2. -49.]] th0 [[11.]]\n",
      "th [[ 12.  17.  14.  17.  22.  10.  -7.  -4.  16. -22.]] th0 [[12.]]\n",
      "th [[ 11.  14.  11.   8.  13.   1. -34. -31. -11. -49.]] th0 [[11.]]\n",
      "th [[ 12.  15.  12.   9.  14.   2. -33. -30. -10. -48.]] th0 [[12.]]\n",
      "th [[ 13.  17.  14.  13.  18.   6. -25. -22.  -2. -40.]] th0 [[13.]]\n",
      "th [[ 14.  19.  17.  17.  24.  15. -17. -10.  16. -13.]] th0 [[14.]]\n",
      "th [[ 15.  23.  18.  33.  28.  16.  47.   6.  20. -12.]] th0 [[15.]]\n",
      "th [[ 14.  22.  15.  32.  25.   7.  46.   3.  11. -39.]] th0 [[14.]]\n",
      "th [[ 13.  19.  12.  23.  16.  -2.  19. -24. -16. -66.]] th0 [[13.]]\n",
      "th [[ 14.  21.  14.  27.  20.   2.  27. -16.  -8. -58.]] th0 [[14.]]\n",
      "th [[ 13.  19.  13.  23.  18.   1.  19. -20. -10. -59.]] th0 [[13.]]\n",
      "th [[ 14.  21.  16.  27.  24.  10.  27.  -8.   8. -32.]] th0 [[14.]]\n",
      "th [[ 13.  18.  13.  18.  15.   1.   0. -35. -19. -59.]] th0 [[13.]]\n",
      "th [[ 14.  19.  14.  19.  16.   2.   1. -34. -18. -58.]] th0 [[14.]]\n",
      "th [[ 15.  21.  16.  23.  20.   6.   9. -26. -10. -50.]] th0 [[15.]]\n",
      "th [[ 14.  19.  15.  19.  18.   5.   1. -30. -12. -51.]] th0 [[14.]]\n",
      "th [[ 15.  21.  18.  23.  24.  14.   9. -18.   6. -24.]] th0 [[15.]]\n",
      "th [[ 14.  20.  16.  22.  22.  10.   8. -20.   2. -32.]] th0 [[14.]]\n",
      "th [[ 13.  18.  15.  18.  20.   9.   0. -24.   0. -33.]] th0 [[13.]]\n",
      "th [[ 14.  20.  18.  22.  26.  18.   8. -12.  18.  -6.]] th0 [[14.]]\n",
      "th [[ 13.  19.  15.  21.  23.   9.   7. -15.   9. -33.]] th0 [[13.]]\n",
      "th [[ 12.  17.  14.  17.  21.   8.  -1. -19.   7. -34.]] th0 [[12.]]\n",
      "th [[13. 19. 17. 21. 27. 17.  7. -7. 25. -7.]] th0 [[13.]]\n",
      "th [[ 12.  18.  14.  20.  24.   8.   6. -10.  16. -34.]] th0 [[12.]]\n",
      "th [[ 11.  16.  13.  16.  22.   7.  -2. -14.  14. -35.]] th0 [[11.]]\n",
      "th [[12. 18. 16. 20. 28. 16.  6. -2. 32. -8.]] th0 [[12.]]\n",
      "th [[ 11.  17.  13.  19.  25.   7.   5.  -5.  23. -35.]] th0 [[11.]]\n",
      "th [[ 10.  14.  10.  10.  16.  -2. -22. -32.  -4. -62.]] th0 [[10.]]\n",
      "th [[ 11.  15.  11.  11.  17.  -1. -21. -31.  -3. -61.]] th0 [[11.]]\n",
      "th [[ 12.  17.  13.  15.  21.   3. -13. -23.   5. -53.]] th0 [[12.]]\n",
      "th [[ 13.  19.  16.  19.  27.  12.  -5. -11.  23. -26.]] th0 [[13.]]\n",
      "th [[ 12.  16.  13.  10.  18.   3. -32. -38.  -4. -53.]] th0 [[12.]]\n",
      "th [[ 13.  17.  14.  11.  19.   4. -31. -37.  -3. -52.]] th0 [[13.]]\n",
      "th [[ 14.  19.  16.  15.  23.   8. -23. -29.   5. -44.]] th0 [[14.]]\n",
      "th [[ 15.  21.  19.  19.  29.  17. -15. -17.  23. -17.]] th0 [[15.]]\n",
      "th [[ 16.  25.  20.  35.  33.  18.  49.  -1.  27. -16.]] th0 [[16.]]\n",
      "th [[ 15.  24.  17.  34.  30.   9.  48.  -4.  18. -43.]] th0 [[15.]]\n",
      "th [[ 14.  21.  14.  25.  21.   0.  21. -31.  -9. -70.]] th0 [[14.]]\n",
      "th [[ 15.  23.  16.  29.  25.   4.  29. -23.  -1. -62.]] th0 [[15.]]\n",
      "th [[ 14.  21.  15.  25.  23.   3.  21. -27.  -3. -63.]] th0 [[14.]]\n",
      "th [[ 15.  23.  18.  29.  29.  12.  29. -15.  15. -36.]] th0 [[15.]]\n",
      "th [[ 14.  20.  15.  20.  20.   3.   2. -42. -12. -63.]] th0 [[14.]]\n",
      "th [[ 15.  21.  16.  21.  21.   4.   3. -41. -11. -62.]] th0 [[15.]]\n",
      "th [[ 16.  23.  18.  25.  25.   8.  11. -33.  -3. -54.]] th0 [[16.]]\n",
      "th [[ 15.  21.  17.  21.  23.   7.   3. -37.  -5. -55.]] th0 [[15.]]\n",
      "th [[ 16.  23.  20.  25.  29.  16.  11. -25.  13. -28.]] th0 [[16.]]\n",
      "th [[ 15.  20.  17.  16.  20.   7. -16. -52. -14. -55.]] th0 [[15.]]\n",
      "th [[ 16.  21.  18.  17.  21.   8. -15. -51. -13. -54.]] th0 [[16.]]\n",
      "th [[ 17.  23.  20.  21.  25.  12.  -7. -43.  -5. -46.]] th0 [[17.]]\n",
      "th [[ 18.  25.  23.  25.  31.  21.   1. -31.  13. -19.]] th0 [[18.]]\n",
      "th [[ 17.  24.  21.  24.  29.  17.   0. -33.   9. -27.]] th0 [[17.]]\n",
      "th [[ 16.  22.  20.  20.  27.  16.  -8. -37.   7. -28.]] th0 [[16.]]\n",
      "th [[ 17.  24.  23.  24.  33.  25.   0. -25.  25.  -1.]] th0 [[17.]]\n",
      "th [[ 16.  23.  20.  23.  30.  16.  -1. -28.  16. -28.]] th0 [[16.]]\n",
      "th [[ 15.  22.  18.  22.  28.  12.  -2. -30.  12. -36.]] th0 [[15.]]\n",
      "th [[ 14.  20.  17.  18.  26.  11. -10. -34.  10. -37.]] th0 [[14.]]\n",
      "th [[ 15.  22.  20.  22.  32.  20.  -2. -22.  28. -10.]] th0 [[15.]]\n",
      "th [[ 14.  21.  17.  21.  29.  11.  -3. -25.  19. -37.]] th0 [[14.]]\n",
      "th [[ 15.  23.  19.  25.  33.  15.   5. -17.  27. -29.]] th0 [[15.]]\n",
      "th [[ 14.  22.  17.  24.  31.  11.   4. -19.  23. -37.]] th0 [[14.]]\n",
      "th [[ 13.  20.  16.  20.  29.  10.  -4. -23.  21. -38.]] th0 [[13.]]\n",
      "th [[ 14.  22.  19.  24.  35.  19.   4. -11.  39. -11.]] th0 [[14.]]\n",
      "th [[ 13.  21.  16.  23.  32.  10.   3. -14.  30. -38.]] th0 [[13.]]\n",
      "th [[ 12.  18.  13.  14.  23.   1. -24. -41.   3. -65.]] th0 [[12.]]\n",
      "th [[ 13.  19.  14.  15.  24.   2. -23. -40.   4. -64.]] th0 [[13.]]\n",
      "th [[ 14.  21.  16.  19.  28.   6. -15. -32.  12. -56.]] th0 [[14.]]\n",
      "th [[ 15.  23.  19.  23.  34.  15.  -7. -20.  30. -29.]] th0 [[15.]]\n",
      "th [[ 16.  27.  20.  39.  38.  16.  57.  -4.  34. -28.]] th0 [[16.]]\n",
      "th [[ 15.  26.  17.  38.  35.   7.  56.  -7.  25. -55.]] th0 [[15.]]\n",
      "th [[ 14.  23.  14.  29.  26.  -2.  29. -34.  -2. -82.]] th0 [[14.]]\n",
      "th [[ 15.  25.  16.  33.  30.   2.  37. -26.   6. -74.]] th0 [[15.]]\n",
      "th [[ 14.  23.  15.  29.  28.   1.  29. -30.   4. -75.]] th0 [[14.]]\n",
      "th [[ 15.  25.  18.  33.  34.  10.  37. -18.  22. -48.]] th0 [[15.]]\n",
      "th [[ 14.  22.  15.  24.  25.   1.  10. -45.  -5. -75.]] th0 [[14.]]\n",
      "th [[ 15.  23.  16.  25.  26.   2.  11. -44.  -4. -74.]] th0 [[15.]]\n",
      "th [[ 16.  25.  18.  29.  30.   6.  19. -36.   4. -66.]] th0 [[16.]]\n",
      "th [[ 15.  23.  17.  25.  28.   5.  11. -40.   2. -67.]] th0 [[15.]]\n",
      "th [[ 16.  25.  20.  29.  34.  14.  19. -28.  20. -40.]] th0 [[16.]]\n",
      "th [[ 15.  22.  17.  20.  25.   5.  -8. -55.  -7. -67.]] th0 [[15.]]\n",
      "th [[ 16.  23.  18.  21.  26.   6.  -7. -54.  -6. -66.]] th0 [[16.]]\n",
      "th [[ 17.  25.  20.  25.  30.  10.   1. -46.   2. -58.]] th0 [[17.]]\n",
      "th [[ 16.  23.  19.  21.  28.   9.  -7. -50.   0. -59.]] th0 [[16.]]\n",
      "th [[ 17.  25.  22.  25.  34.  18.   1. -38.  18. -32.]] th0 [[17.]]\n",
      "th [[ 16.  24.  20.  24.  32.  14.   0. -40.  14. -40.]] th0 [[16.]]\n",
      "th [[ 15.  22.  19.  20.  30.  13.  -8. -44.  12. -41.]] th0 [[15.]]\n",
      "th [[ 16.  24.  22.  24.  36.  22.   0. -32.  30. -14.]] th0 [[16.]]\n",
      "th [[ 15.  23.  19.  23.  33.  13.  -1. -35.  21. -41.]] th0 [[15.]]\n",
      "th [[ 16.  25.  21.  27.  37.  17.   7. -27.  29. -33.]] th0 [[16.]]\n",
      "th [[ 15.  24.  19.  26.  35.  13.   6. -29.  25. -41.]] th0 [[15.]]\n",
      "th [[ 14.  22.  18.  22.  33.  12.  -2. -33.  23. -42.]] th0 [[14.]]\n",
      "th [[ 15.  24.  21.  26.  39.  21.   6. -21.  41. -15.]] th0 [[15.]]\n",
      "th [[ 14.  23.  18.  25.  36.  12.   5. -24.  32. -42.]] th0 [[14.]]\n",
      "th [[ 13.  20.  15.  16.  27.   3. -22. -51.   5. -69.]] th0 [[13.]]\n",
      "th [[ 14.  21.  16.  17.  28.   4. -21. -50.   6. -68.]] th0 [[14.]]\n",
      "th [[ 15.  23.  18.  21.  32.   8. -13. -42.  14. -60.]] th0 [[15.]]\n",
      "th [[ 16.  25.  21.  25.  38.  17.  -5. -30.  32. -33.]] th0 [[16.]]\n",
      "th [[ 15.  24.  19.  24.  36.  13.  -6. -32.  28. -41.]] th0 [[15.]]\n",
      "th [[ 14.  22.  18.  20.  34.  12. -14. -36.  26. -42.]] th0 [[14.]]\n",
      "th [[ 15.  24.  21.  24.  40.  21.  -6. -24.  44. -15.]] th0 [[15.]]\n",
      "th [[ 14.  23.  18.  23.  37.  12.  -7. -27.  35. -42.]] th0 [[14.]]\n",
      "th [[ 13.  21.  17.  19.  35.  11. -15. -31.  33. -43.]] th0 [[13.]]\n",
      "th [[ 14.  23.  20.  23.  41.  20.  -7. -19.  51. -16.]] th0 [[14.]]\n",
      "th [[ 13.  22.  17.  22.  38.  11.  -8. -22.  42. -43.]] th0 [[13.]]\n",
      "th [[ 12.  20.  16.  18.  36.  10. -16. -26.  40. -44.]] th0 [[12.]]\n",
      "th [[ 13.  22.  19.  22.  42.  19.  -8. -14.  58. -17.]] th0 [[13.]]\n",
      "th [[ 12.  21.  16.  21.  39.  10.  -9. -17.  49. -44.]] th0 [[12.]]\n",
      "th [[ 11.  18.  13.  12.  30.   1. -36. -44.  22. -71.]] th0 [[11.]]\n",
      "th [[ 12.  19.  14.  13.  31.   2. -35. -43.  23. -70.]] th0 [[12.]]\n",
      "th [[ 13.  21.  16.  17.  35.   6. -27. -35.  31. -62.]] th0 [[13.]]\n",
      "th [[ 14.  23.  19.  21.  41.  15. -19. -23.  49. -35.]] th0 [[14.]]\n",
      "th [[ 15.  27.  20.  37.  45.  16.  45.  -7.  53. -34.]] th0 [[15.]]\n",
      "th [[ 14.  26.  17.  36.  42.   7.  44. -10.  44. -61.]] th0 [[14.]]\n",
      "th [[ 13.  23.  14.  27.  33.  -2.  17. -37.  17. -88.]] th0 [[13.]]\n",
      "th [[ 14.  25.  16.  31.  37.   2.  25. -29.  25. -80.]] th0 [[14.]]\n",
      "th [[ 13.  23.  15.  27.  35.   1.  17. -33.  23. -81.]] th0 [[13.]]\n",
      "th [[ 14.  25.  18.  31.  41.  10.  25. -21.  41. -54.]] th0 [[14.]]\n",
      "th [[ 13.  22.  15.  22.  32.   1.  -2. -48.  14. -81.]] th0 [[13.]]\n",
      "th [[ 14.  24.  17.  26.  36.   5.   6. -40.  22. -73.]] th0 [[14.]]\n",
      "th [[ 13.  22.  16.  22.  34.   4.  -2. -44.  20. -74.]] th0 [[13.]]\n",
      "th [[ 14.  24.  19.  26.  40.  13.   6. -32.  38. -47.]] th0 [[14.]]\n",
      "th [[ 13.  22.  18.  22.  38.  12.  -2. -36.  36. -48.]] th0 [[13.]]\n",
      "th [[ 14.  24.  21.  26.  44.  21.   6. -24.  54. -21.]] th0 [[14.]]\n",
      "th [[ 13.  23.  18.  25.  41.  12.   5. -27.  45. -48.]] th0 [[13.]]\n",
      "th [[ 12.  20.  15.  16.  32.   3. -22. -54.  18. -75.]] th0 [[12.]]\n",
      "th [[ 13.  21.  16.  17.  33.   4. -21. -53.  19. -74.]] th0 [[13.]]\n",
      "th [[ 14.  23.  18.  21.  37.   8. -13. -45.  27. -66.]] th0 [[14.]]\n",
      "th [[ 15.  25.  21.  25.  43.  17.  -5. -33.  45. -39.]] th0 [[15.]]\n",
      "th [[ 14.  22.  18.  16.  34.   8. -32. -60.  18. -66.]] th0 [[14.]]\n",
      "th [[ 15.  23.  19.  17.  35.   9. -31. -59.  19. -65.]] th0 [[15.]]\n",
      "th [[ 16.  25.  21.  21.  39.  13. -23. -51.  27. -57.]] th0 [[16.]]\n",
      "th [[ 17.  27.  24.  25.  45.  22. -15. -39.  45. -30.]] th0 [[17.]]\n",
      "th [[ 18.  31.  25.  41.  49.  23.  49. -23.  49. -29.]] th0 [[18.]]\n",
      "th [[ 17.  30.  22.  40.  46.  14.  48. -26.  40. -56.]] th0 [[17.]]\n",
      "th [[ 16.  27.  19.  31.  37.   5.  21. -53.  13. -83.]] th0 [[16.]]\n",
      "th [[ 17.  29.  21.  35.  41.   9.  29. -45.  21. -75.]] th0 [[17.]]\n",
      "th [[ 16.  27.  20.  31.  39.   8.  21. -49.  19. -76.]] th0 [[16.]]\n",
      "th [[ 17.  29.  23.  35.  45.  17.  29. -37.  37. -49.]] th0 [[17.]]\n",
      "th [[ 16.  26.  20.  26.  36.   8.   2. -64.  10. -76.]] th0 [[16.]]\n",
      "th [[ 17.  28.  22.  30.  40.  12.  10. -56.  18. -68.]] th0 [[17.]]\n",
      "th [[ 16.  26.  21.  26.  38.  11.   2. -60.  16. -69.]] th0 [[16.]]\n",
      "th [[ 17.  28.  24.  30.  44.  20.  10. -48.  34. -42.]] th0 [[17.]]\n",
      "th [[ 16.  27.  22.  29.  42.  16.   9. -50.  30. -50.]] th0 [[16.]]\n",
      "th [[ 15.  25.  21.  25.  40.  15.   1. -54.  28. -51.]] th0 [[15.]]\n",
      "th [[ 16.  27.  24.  29.  46.  24.   9. -42.  46. -24.]] th0 [[16.]]\n",
      "th [[ 15.  26.  21.  28.  43.  15.   8. -45.  37. -51.]] th0 [[15.]]\n",
      "th [[ 14.  24.  20.  24.  41.  14.   0. -49.  35. -52.]] th0 [[14.]]\n",
      "th [[ 15.  26.  23.  28.  47.  23.   8. -37.  53. -25.]] th0 [[15.]]\n",
      "th [[ 14.  25.  20.  27.  44.  14.   7. -40.  44. -52.]] th0 [[14.]]\n",
      "th [[ 13.  23.  19.  23.  42.  13.  -1. -44.  42. -53.]] th0 [[13.]]\n",
      "th [[ 14.  25.  22.  27.  48.  22.   7. -32.  60. -26.]] th0 [[14.]]\n",
      "th [[ 13.  24.  19.  26.  45.  13.   6. -35.  51. -53.]] th0 [[13.]]\n",
      "th [[ 12.  21.  16.  17.  36.   4. -21. -62.  24. -80.]] th0 [[12.]]\n",
      "th [[ 13.  22.  17.  18.  37.   5. -20. -61.  25. -79.]] th0 [[13.]]\n",
      "th [[ 14.  24.  19.  22.  41.   9. -12. -53.  33. -71.]] th0 [[14.]]\n",
      "th [[ 15.  26.  22.  26.  47.  18.  -4. -41.  51. -44.]] th0 [[15.]]\n",
      "th [[ 14.  25.  20.  25.  45.  14.  -5. -43.  47. -52.]] th0 [[14.]]\n",
      "th [[ 13.  23.  19.  21.  43.  13. -13. -47.  45. -53.]] th0 [[13.]]\n",
      "th [[ 14.  25.  22.  25.  49.  22.  -5. -35.  63. -26.]] th0 [[14.]]\n",
      "th [[ 13.  24.  19.  24.  46.  13.  -6. -38.  54. -53.]] th0 [[13.]]\n",
      "th [[ 12.  22.  18.  20.  44.  12. -14. -42.  52. -54.]] th0 [[12.]]\n",
      "th [[ 13.  24.  21.  24.  50.  21.  -6. -30.  70. -27.]] th0 [[13.]]\n",
      "th [[ 12.  23.  18.  23.  47.  12.  -7. -33.  61. -54.]] th0 [[12.]]\n",
      "th [[ 11.  21.  17.  19.  45.  11. -15. -37.  59. -55.]] th0 [[11.]]\n",
      "th [[ 12.  23.  20.  23.  51.  20.  -7. -25.  77. -28.]] th0 [[12.]]\n",
      "th [[ 11.  22.  17.  22.  48.  11.  -8. -28.  68. -55.]] th0 [[11.]]\n",
      "th [[ 10.  19.  14.  13.  39.   2. -35. -55.  41. -82.]] th0 [[10.]]\n",
      "th [[ 11.  20.  15.  14.  40.   3. -34. -54.  42. -81.]] th0 [[11.]]\n",
      "th [[ 12.  22.  17.  18.  44.   7. -26. -46.  50. -73.]] th0 [[12.]]\n",
      "th [[ 13.  24.  20.  22.  50.  16. -18. -34.  68. -46.]] th0 [[13.]]\n",
      "th [[ 14.  28.  21.  38.  54.  17.  46. -18.  72. -45.]] th0 [[14.]]\n",
      "th [[ 13.  25.  18.  29.  45.   8.  19. -45.  45. -72.]] th0 [[13.]]\n",
      "th [[ 12.  23.  17.  25.  43.   7.  11. -49.  43. -73.]] th0 [[12.]]\n",
      "th [[ 13.  25.  20.  29.  49.  16.  19. -37.  61. -46.]] th0 [[13.]]\n",
      "th [[ 12.  22.  17.  20.  40.   7.  -8. -64.  34. -73.]] th0 [[12.]]\n",
      "th [[ 13.  24.  19.  24.  44.  11.   0. -56.  42. -65.]] th0 [[13.]]\n",
      "th [[ 12.  22.  18.  20.  42.  10.  -8. -60.  40. -66.]] th0 [[12.]]\n",
      "th [[ 13.  24.  21.  24.  48.  19.   0. -48.  58. -39.]] th0 [[13.]]\n",
      "th [[ 12.  21.  18.  15.  39.  10. -27. -75.  31. -66.]] th0 [[12.]]\n",
      "th [[ 13.  22.  19.  16.  40.  11. -26. -74.  32. -65.]] th0 [[13.]]\n",
      "th [[ 14.  24.  21.  20.  44.  15. -18. -66.  40. -57.]] th0 [[14.]]\n",
      "th [[ 15.  26.  24.  24.  50.  24. -10. -54.  58. -30.]] th0 [[15.]]\n",
      "th [[ 16.  30.  25.  40.  54.  25.  54. -38.  62. -29.]] th0 [[16.]]\n",
      "th [[ 15.  29.  22.  39.  51.  16.  53. -41.  53. -56.]] th0 [[15.]]\n",
      "th [[ 14.  26.  19.  30.  42.   7.  26. -68.  26. -83.]] th0 [[14.]]\n",
      "th [[ 15.  28.  21.  34.  46.  11.  34. -60.  34. -75.]] th0 [[15.]]\n",
      "th [[ 14.  26.  20.  30.  44.  10.  26. -64.  32. -76.]] th0 [[14.]]\n",
      "th [[ 15.  28.  23.  34.  50.  19.  34. -52.  50. -49.]] th0 [[15.]]\n",
      "th [[ 14.  25.  20.  25.  41.  10.   7. -79.  23. -76.]] th0 [[14.]]\n",
      "th [[ 15.  27.  22.  29.  45.  14.  15. -71.  31. -68.]] th0 [[15.]]\n",
      "th [[ 14.  25.  21.  25.  43.  13.   7. -75.  29. -69.]] th0 [[14.]]\n",
      "th [[ 15.  27.  24.  29.  49.  22.  15. -63.  47. -42.]] th0 [[15.]]\n",
      "th [[ 14.  26.  22.  28.  47.  18.  14. -65.  43. -50.]] th0 [[14.]]\n",
      "th [[ 13.  24.  21.  24.  45.  17.   6. -69.  41. -51.]] th0 [[13.]]\n",
      "th [[ 14.  26.  24.  28.  51.  26.  14. -57.  59. -24.]] th0 [[14.]]\n",
      "th [[ 13.  25.  21.  27.  48.  17.  13. -60.  50. -51.]] th0 [[13.]]\n",
      "th [[ 12.  23.  20.  23.  46.  16.   5. -64.  48. -52.]] th0 [[12.]]\n",
      "th [[ 13.  25.  23.  27.  52.  25.  13. -52.  66. -25.]] th0 [[13.]]\n",
      "th [[ 12.  24.  20.  26.  49.  16.  12. -55.  57. -52.]] th0 [[12.]]\n",
      "th [[ 11.  22.  19.  22.  47.  15.   4. -59.  55. -53.]] th0 [[11.]]\n",
      "th [[ 12.  24.  22.  26.  53.  24.  12. -47.  73. -26.]] th0 [[12.]]\n",
      "th [[ 11.  23.  19.  25.  50.  15.  11. -50.  64. -53.]] th0 [[11.]]\n",
      "th [[ 10.  20.  16.  16.  41.   6. -16. -77.  37. -80.]] th0 [[10.]]\n",
      "th [[ 11.  21.  17.  17.  42.   7. -15. -76.  38. -79.]] th0 [[11.]]\n",
      "th [[ 12.  23.  19.  21.  46.  11.  -7. -68.  46. -71.]] th0 [[12.]]\n",
      "th [[ 13.  25.  22.  25.  52.  20.   1. -56.  64. -44.]] th0 [[13.]]\n",
      "th [[ 12.  22.  19.  16.  43.  11. -26. -83.  37. -71.]] th0 [[12.]]\n",
      "th [[ 13.  23.  20.  17.  44.  12. -25. -82.  38. -70.]] th0 [[13.]]\n",
      "th [[ 14.  25.  22.  21.  48.  16. -17. -74.  46. -62.]] th0 [[14.]]\n",
      "th [[ 15.  27.  25.  25.  54.  25.  -9. -62.  64. -35.]] th0 [[15.]]\n",
      "th [[ 16.  31.  26.  41.  58.  26.  55. -46.  68. -34.]] th0 [[16.]]\n",
      "th [[ 15.  30.  23.  40.  55.  17.  54. -49.  59. -61.]] th0 [[15.]]\n",
      "th [[ 14.  27.  20.  31.  46.   8.  27. -76.  32. -88.]] th0 [[14.]]\n",
      "th [[ 15.  29.  22.  35.  50.  12.  35. -68.  40. -80.]] th0 [[15.]]\n",
      "th [[ 14.  27.  21.  31.  48.  11.  27. -72.  38. -81.]] th0 [[14.]]\n",
      "th [[ 15.  29.  24.  35.  54.  20.  35. -60.  56. -54.]] th0 [[15.]]\n",
      "th [[ 14.  26.  21.  26.  45.  11.   8. -87.  29. -81.]] th0 [[14.]]\n",
      "th [[ 15.  28.  23.  30.  49.  15.  16. -79.  37. -73.]] th0 [[15.]]\n",
      "th [[ 14.  26.  22.  26.  47.  14.   8. -83.  35. -74.]] th0 [[14.]]\n",
      "th [[ 15.  28.  25.  30.  53.  23.  16. -71.  53. -47.]] th0 [[15.]]\n",
      "th [[ 14.  27.  23.  29.  51.  19.  15. -73.  49. -55.]] th0 [[14.]]\n",
      "th [[ 13.  25.  22.  25.  49.  18.   7. -77.  47. -56.]] th0 [[13.]]\n",
      "th [[ 14.  27.  25.  29.  55.  27.  15. -65.  65. -29.]] th0 [[14.]]\n",
      "th [[ 13.  26.  22.  28.  52.  18.  14. -68.  56. -56.]] th0 [[13.]]\n",
      "th [[ 12.  24.  21.  24.  50.  17.   6. -72.  54. -57.]] th0 [[12.]]\n",
      "th [[ 13.  26.  24.  28.  56.  26.  14. -60.  72. -30.]] th0 [[13.]]\n",
      "th [[ 12.  25.  21.  27.  53.  17.  13. -63.  63. -57.]] th0 [[12.]]\n",
      "th [[ 11.  23.  20.  23.  51.  16.   5. -67.  61. -58.]] th0 [[11.]]\n",
      "th [[ 12.  25.  23.  27.  57.  25.  13. -55.  79. -31.]] th0 [[12.]]\n",
      "th [[ 11.  24.  20.  26.  54.  16.  12. -58.  70. -58.]] th0 [[11.]]\n",
      "th [[ 10.  21.  17.  17.  45.   7. -15. -85.  43. -85.]] th0 [[10.]]\n",
      "th [[ 11.  22.  18.  18.  46.   8. -14. -84.  44. -84.]] th0 [[11.]]\n",
      "th [[ 12.  24.  20.  22.  50.  12.  -6. -76.  52. -76.]] th0 [[12.]]\n",
      "th [[ 13.  26.  23.  26.  56.  21.   2. -64.  70. -49.]] th0 [[13.]]\n",
      "th [[ 12.  25.  21.  25.  54.  17.   1. -66.  66. -57.]] th0 [[12.]]\n",
      "th [[ 11.  23.  20.  21.  52.  16.  -7. -70.  64. -58.]] th0 [[11.]]\n",
      "th [[ 12.  25.  23.  25.  58.  25.   1. -58.  82. -31.]] th0 [[12.]]\n",
      "th [[ 11.  24.  20.  24.  55.  16.   0. -61.  73. -58.]] th0 [[11.]]\n",
      "th [[ 10.  22.  19.  20.  53.  15.  -8. -65.  71. -59.]] th0 [[10.]]\n",
      "th [[ 11.  24.  22.  24.  59.  24.   0. -53.  89. -32.]] th0 [[11.]]\n",
      "th [[ 10.  23.  19.  23.  56.  15.  -1. -56.  80. -59.]] th0 [[10.]]\n",
      "th [[  9.  20.  16.  14.  47.   6. -28. -83.  53. -86.]] th0 [[9.]]\n",
      "th [[ 10.  21.  17.  15.  48.   7. -27. -82.  54. -85.]] th0 [[10.]]\n",
      "th [[ 11.  23.  19.  19.  52.  11. -19. -74.  62. -77.]] th0 [[11.]]\n",
      "th [[ 12.  25.  22.  23.  58.  20. -11. -62.  80. -50.]] th0 [[12.]]\n",
      "th [[ 13.  29.  23.  39.  62.  21.  53. -46.  84. -49.]] th0 [[13.]]\n",
      "th [[ 12.  26.  20.  30.  53.  12.  26. -73.  57. -76.]] th0 [[12.]]\n",
      "th [[ 13.  28.  22.  34.  57.  16.  34. -65.  65. -68.]] th0 [[13.]]\n",
      "th [[ 12.  26.  21.  30.  55.  15.  26. -69.  63. -69.]] th0 [[12.]]\n",
      "th [[ 13.  28.  24.  34.  61.  24.  34. -57.  81. -42.]] th0 [[13.]]\n",
      "th [[ 12.  27.  21.  33.  58.  15.  33. -60.  72. -69.]] th0 [[12.]]\n",
      "th [[ 11.  24.  18.  24.  49.   6.   6. -87.  45. -96.]] th0 [[11.]]\n",
      "th [[ 12.  26.  20.  28.  53.  10.  14. -79.  53. -88.]] th0 [[12.]]\n",
      "th [[ 11.  24.  19.  24.  51.   9.   6. -83.  51. -89.]] th0 [[11.]]\n",
      "th [[ 12.  26.  22.  28.  57.  18.  14. -71.  69. -62.]] th0 [[12.]]\n",
      "th [[ 11.  24.  21.  24.  55.  17.   6. -75.  67. -63.]] th0 [[11.]]\n",
      "th [[ 12.  26.  24.  28.  61.  26.  14. -63.  85. -36.]] th0 [[12.]]\n",
      "th [[ 11.  25.  21.  27.  58.  17.  13. -66.  76. -63.]] th0 [[11.]]\n",
      "th [[ 10.  23.  20.  23.  56.  16.   5. -70.  74. -64.]] th0 [[10.]]\n",
      "th [[ 11.  25.  23.  27.  62.  25.  13. -58.  92. -37.]] th0 [[11.]]\n",
      "th [[ 10.  24.  20.  26.  59.  16.  12. -61.  83. -64.]] th0 [[10.]]\n",
      "th [[  9.  21.  17.  17.  50.   7. -15. -88.  56. -91.]] th0 [[9.]]\n",
      "th [[ 10.  22.  18.  18.  51.   8. -14. -87.  57. -90.]] th0 [[10.]]\n",
      "th [[ 11.  24.  20.  22.  55.  12.  -6. -79.  65. -82.]] th0 [[11.]]\n",
      "th [[ 12.  26.  23.  26.  61.  21.   2. -67.  83. -55.]] th0 [[12.]]\n",
      "th [[ 11.  23.  20.  17.  52.  12. -25. -94.  56. -82.]] th0 [[11.]]\n",
      "th [[ 12.  25.  22.  21.  56.  16. -17. -86.  64. -74.]] th0 [[12.]]\n",
      "th [[ 13.  27.  25.  25.  62.  25.  -9. -74.  82. -47.]] th0 [[13.]]\n",
      "th [[ 14.  31.  26.  41.  66.  26.  55. -58.  86. -46.]] th0 [[14.]]\n",
      "th [[ 13.  30.  23.  40.  63.  17.  54. -61.  77. -73.]] th0 [[13.]]\n",
      "th [[  12.   27.   20.   31.   54.    8.   27.  -88.   50. -100.]] th0 [[12.]]\n",
      "th [[ 13.  29.  22.  35.  58.  12.  35. -80.  58. -92.]] th0 [[13.]]\n",
      "th [[ 12.  27.  21.  31.  56.  11.  27. -84.  56. -93.]] th0 [[12.]]\n",
      "th [[ 13.  29.  24.  35.  62.  20.  35. -72.  74. -66.]] th0 [[13.]]\n",
      "th [[ 12.  26.  21.  26.  53.  11.   8. -99.  47. -93.]] th0 [[12.]]\n",
      "th [[ 13.  28.  23.  30.  57.  15.  16. -91.  55. -85.]] th0 [[13.]]\n",
      "th [[ 12.  26.  22.  26.  55.  14.   8. -95.  53. -86.]] th0 [[12.]]\n",
      "th [[ 13.  28.  25.  30.  61.  23.  16. -83.  71. -59.]] th0 [[13.]]\n",
      "th [[ 12.  27.  23.  29.  59.  19.  15. -85.  67. -67.]] th0 [[12.]]\n",
      "th [[ 11.  25.  22.  25.  57.  18.   7. -89.  65. -68.]] th0 [[11.]]\n",
      "th [[ 12.  27.  25.  29.  63.  27.  15. -77.  83. -41.]] th0 [[12.]]\n",
      "th [[ 11.  26.  22.  28.  60.  18.  14. -80.  74. -68.]] th0 [[11.]]\n",
      "th [[ 10.  24.  21.  24.  58.  17.   6. -84.  72. -69.]] th0 [[10.]]\n",
      "th [[ 11.  26.  24.  28.  64.  26.  14. -72.  90. -42.]] th0 [[11.]]\n",
      "th [[ 10.  25.  21.  27.  61.  17.  13. -75.  81. -69.]] th0 [[10.]]\n",
      "th [[  9.  23.  20.  23.  59.  16.   5. -79.  79. -70.]] th0 [[9.]]\n",
      "th [[ 10.  25.  23.  27.  65.  25.  13. -67.  97. -43.]] th0 [[10.]]\n",
      "th [[  9.  24.  20.  26.  62.  16.  12. -70.  88. -70.]] th0 [[9.]]\n",
      "th [[  8.  21.  17.  17.  53.   7. -15. -97.  61. -97.]] th0 [[8.]]\n",
      "th [[  9.  22.  18.  18.  54.   8. -14. -96.  62. -96.]] th0 [[9.]]\n",
      "th [[ 10.  24.  20.  22.  58.  12.  -6. -88.  70. -88.]] th0 [[10.]]\n",
      "th [[ 11.  26.  23.  26.  64.  21.   2. -76.  88. -61.]] th0 [[11.]]\n",
      "th [[ 10.  25.  21.  25.  62.  17.   1. -78.  84. -69.]] th0 [[10.]]\n",
      "th [[  9.  23.  20.  21.  60.  16.  -7. -82.  82. -70.]] th0 [[9.]]\n",
      "th [[ 10.  25.  23.  25.  66.  25.   1. -70. 100. -43.]] th0 [[10.]]\n",
      "th [[  9.  24.  20.  24.  63.  16.   0. -73.  91. -70.]] th0 [[9.]]\n",
      "th [[  8.  22.  19.  20.  61.  15.  -8. -77.  89. -71.]] th0 [[8.]]\n",
      "th [[  9.  24.  22.  24.  67.  24.   0. -65. 107. -44.]] th0 [[9.]]\n",
      "th [[  8.  23.  19.  23.  64.  15.  -1. -68.  98. -71.]] th0 [[8.]]\n",
      "th [[  7.  21.  18.  19.  62.  14.  -9. -72.  96. -72.]] th0 [[7.]]\n",
      "th [[  8.  23.  21.  23.  68.  23.  -1. -60. 114. -45.]] th0 [[8.]]\n",
      "th [[  7.  22.  18.  22.  65.  14.  -2. -63. 105. -72.]] th0 [[7.]]\n",
      "th [[  6.  19.  15.  13.  56.   5. -29. -90.  78. -99.]] th0 [[6.]]\n",
      "th [[  7.  20.  16.  14.  57.   6. -28. -89.  79. -98.]] th0 [[7.]]\n",
      "th [[  8.  22.  18.  18.  61.  10. -20. -81.  87. -90.]] th0 [[8.]]\n",
      "th [[  9.  24.  21.  22.  67.  19. -12. -69. 105. -63.]] th0 [[9.]]\n",
      "th [[ 10.  28.  22.  38.  71.  20.  52. -53. 109. -62.]] th0 [[10.]]\n",
      "th [[  9.  25.  19.  29.  62.  11.  25. -80.  82. -89.]] th0 [[9.]]\n",
      "th [[  8.  23.  18.  25.  60.  10.  17. -84.  80. -90.]] th0 [[8.]]\n",
      "th [[  9.  25.  21.  29.  66.  19.  25. -72.  98. -63.]] th0 [[9.]]\n",
      "th [[  8.  22.  18.  20.  57.  10.  -2. -99.  71. -90.]] th0 [[8.]]\n",
      "th [[  9.  24.  20.  24.  61.  14.   6. -91.  79. -82.]] th0 [[9.]]\n",
      "th [[  8.  22.  19.  20.  59.  13.  -2. -95.  77. -83.]] th0 [[8.]]\n",
      "th [[  9.  24.  22.  24.  65.  22.   6. -83.  95. -56.]] th0 [[9.]]\n",
      "th [[   8.   21.   19.   15.   56.   13.  -21. -110.   68.  -83.]] th0 [[8.]]\n",
      "th [[   9.   22.   20.   16.   57.   14.  -20. -109.   69.  -82.]] th0 [[9.]]\n",
      "th [[  10.   24.   22.   20.   61.   18.  -12. -101.   77.  -74.]] th0 [[10.]]\n",
      "th [[ 11.  26.  25.  24.  67.  27.  -4. -89.  95. -47.]] th0 [[11.]]\n",
      "th [[ 12.  30.  26.  40.  71.  28.  60. -73.  99. -46.]] th0 [[12.]]\n",
      "th [[ 11.  29.  23.  39.  68.  19.  59. -76.  90. -73.]] th0 [[11.]]\n",
      "th [[  10.   26.   20.   30.   59.   10.   32. -103.   63. -100.]] th0 [[10.]]\n",
      "th [[ 11.  28.  22.  34.  63.  14.  40. -95.  71. -92.]] th0 [[11.]]\n",
      "th [[ 10.  26.  21.  30.  61.  13.  32. -99.  69. -93.]] th0 [[10.]]\n",
      "th [[ 11.  28.  24.  34.  67.  22.  40. -87.  87. -66.]] th0 [[11.]]\n",
      "th [[  10.   25.   21.   25.   58.   13.   13. -114.   60.  -93.]] th0 [[10.]]\n",
      "th [[  11.   27.   23.   29.   62.   17.   21. -106.   68.  -85.]] th0 [[11.]]\n",
      "th [[  10.   25.   22.   25.   60.   16.   13. -110.   66.  -86.]] th0 [[10.]]\n",
      "th [[ 11.  27.  25.  29.  66.  25.  21. -98.  84. -59.]] th0 [[11.]]\n",
      "th [[  10.   26.   23.   28.   64.   21.   20. -100.   80.  -67.]] th0 [[10.]]\n",
      "th [[   9.   24.   22.   24.   62.   20.   12. -104.   78.  -68.]] th0 [[9.]]\n",
      "th [[ 10.  26.  25.  28.  68.  29.  20. -92.  96. -41.]] th0 [[10.]]\n",
      "th [[  9.  25.  22.  27.  65.  20.  19. -95.  87. -68.]] th0 [[9.]]\n",
      "th [[  8.  23.  21.  23.  63.  19.  11. -99.  85. -69.]] th0 [[8.]]\n",
      "th [[  9.  25.  24.  27.  69.  28.  19. -87. 103. -42.]] th0 [[9.]]\n",
      "th [[  8.  24.  21.  26.  66.  19.  18. -90.  94. -69.]] th0 [[8.]]\n",
      "th [[  7.  22.  20.  22.  64.  18.  10. -94.  92. -70.]] th0 [[7.]]\n",
      "th [[  8.  24.  23.  26.  70.  27.  18. -82. 110. -43.]] th0 [[8.]]\n",
      "th [[  7.  23.  20.  25.  67.  18.  17. -85. 101. -70.]] th0 [[7.]]\n",
      "th [[   6.   20.   17.   16.   58.    9.  -10. -112.   74.  -97.]] th0 [[6.]]\n",
      "th [[   7.   21.   18.   17.   59.   10.   -9. -111.   75.  -96.]] th0 [[7.]]\n",
      "th [[   8.   23.   20.   21.   63.   14.   -1. -103.   83.  -88.]] th0 [[8.]]\n",
      "th [[  9.  25.  23.  25.  69.  23.   7. -91. 101. -61.]] th0 [[9.]]\n",
      "th [[   8.   22.   20.   16.   60.   14.  -20. -118.   74.  -88.]] th0 [[8.]]\n",
      "th [[   9.   23.   21.   17.   61.   15.  -19. -117.   75.  -87.]] th0 [[9.]]\n",
      "th [[  10.   25.   23.   21.   65.   19.  -11. -109.   83.  -79.]] th0 [[10.]]\n",
      "th [[ 11.  27.  26.  25.  71.  28.  -3. -97. 101. -52.]] th0 [[11.]]\n",
      "th [[ 12.  31.  27.  41.  75.  29.  61. -81. 105. -51.]] th0 [[12.]]\n",
      "th [[ 11.  30.  24.  40.  72.  20.  60. -84.  96. -78.]] th0 [[11.]]\n",
      "th [[  10.   27.   21.   31.   63.   11.   33. -111.   69. -105.]] th0 [[10.]]\n",
      "th [[  11.   29.   23.   35.   67.   15.   41. -103.   77.  -97.]] th0 [[11.]]\n",
      "th [[  10.   27.   22.   31.   65.   14.   33. -107.   75.  -98.]] th0 [[10.]]\n",
      "th [[ 11.  29.  25.  35.  71.  23.  41. -95.  93. -71.]] th0 [[11.]]\n",
      "th [[  10.   26.   22.   26.   62.   14.   14. -122.   66.  -98.]] th0 [[10.]]\n",
      "th [[  11.   28.   24.   30.   66.   18.   22. -114.   74.  -90.]] th0 [[11.]]\n",
      "th [[  10.   26.   23.   26.   64.   17.   14. -118.   72.  -91.]] th0 [[10.]]\n",
      "th [[  11.   28.   26.   30.   70.   26.   22. -106.   90.  -64.]] th0 [[11.]]\n",
      "th [[  10.   27.   24.   29.   68.   22.   21. -108.   86.  -72.]] th0 [[10.]]\n",
      "th [[   9.   25.   23.   25.   66.   21.   13. -112.   84.  -73.]] th0 [[9.]]\n",
      "th [[  10.   27.   26.   29.   72.   30.   21. -100.  102.  -46.]] th0 [[10.]]\n",
      "th [[   9.   26.   23.   28.   69.   21.   20. -103.   93.  -73.]] th0 [[9.]]\n",
      "th [[   9.   26.   23.   28.   69.   21.   20. -103.   93.  -73.]] th0 [[9.]]\n",
      "Final score 6\n",
      "Params [[   9.   26.   23.   28.   69.   21.   20. -103.   93.  -73.]] [[9.]]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[1, 1, 2, 2],\n",
    "                 [1, 2, 1, 2]])\n",
    "labels = np.array([[-1, 1, 1, -1]])\n",
    "\n",
    "def perceptron(data, labels, params = {}, hook = None):\n",
    "    T = params.get('T', 100)\n",
    "    (d, n) = data.shape\n",
    "    m = 0\n",
    "    theta = np.zeros((d, 1))\n",
    "    theta_0 = np.zeros((1, 1))\n",
    "    for t in range(T):\n",
    "        for i in range(n):\n",
    "            x = data[:,i:i+1]\n",
    "            y = labels[:,i:i+1]\n",
    "            if y * positive(x, theta, theta_0) <= 0.0:\n",
    "                m += 1\n",
    "                theta = theta + y * x\n",
    "                theta_0 = theta_0 + y\n",
    "                if hook: hook((theta, theta_0))\n",
    "    return theta, theta_0\n",
    "\n",
    "test_with_features(xor_more,3, draw=True, pause=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
