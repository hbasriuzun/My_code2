{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing code_for_hw03 (part 2, imported as hw3)\n",
      "Imported tidy_plot, plot_separator, plot_data, plot_nonlin_sep, cv, rv, y, positive, score\n",
      "         xval_learning_alg, eval_classifier\n",
      "Tests: test_linear_classifier\n",
      "Dataset tools: load_auto_data, std_vals, standard, raw, one_hot, auto_data_and_labels\n",
      "               load_review_data, clean, extract_words, bag_of_words, extract_bow_feature_vectors\n",
      "               load_mnist_data, load_mnist_single\n"
     ]
    }
   ],
   "source": [
    "# Implement perceptron, average perceptron, and pegasos\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.image import imread\n",
    "\n",
    "\n",
    "print(\"Importing code_for_hw03 (part 2, imported as hw3)\")\n",
    "\n",
    "######################################################################\n",
    "# Plotting\n",
    "\n",
    "def tidy_plot(xmin, xmax, ymin, ymax, center = False, title = None,\n",
    "                 xlabel = None, ylabel = None):\n",
    "    plt.ion()\n",
    "    plt.figure(facecolor=\"white\")\n",
    "    ax = plt.subplot()\n",
    "    if center:\n",
    "        ax.spines['left'].set_position('zero')\n",
    "        ax.spines['right'].set_color('none')\n",
    "        ax.spines['bottom'].set_position('zero')\n",
    "        ax.spines['top'].set_color('none')\n",
    "        ax.spines['left'].set_smart_bounds(True)\n",
    "        ax.spines['bottom'].set_smart_bounds(True)\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "    else:\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "    eps = .05\n",
    "    plt.xlim(xmin-eps, xmax+eps)\n",
    "    plt.ylim(ymin-eps, ymax+eps)\n",
    "    if title: ax.set_title(title)\n",
    "    if xlabel: ax.set_xlabel(xlabel)\n",
    "    if ylabel: ax.set_ylabel(ylabel)\n",
    "    return ax\n",
    "\n",
    "def plot_separator(ax, th, th_0):\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    ymin,ymax = ax.get_ylim()\n",
    "    pts = []\n",
    "    eps = 1.0e-6\n",
    "    # xmin boundary crossing is when xmin th[0] + y th[1] + th_0 = 0\n",
    "    # that is, y = (-th_0 - xmin th[0]) / th[1]\n",
    "    if abs(th[1,0]) > eps:\n",
    "        pts += [np.array([x, (-th_0 - x * th[0,0]) / th[1,0]]) \\\n",
    "                                                        for x in (xmin, xmax)]\n",
    "    if abs(th[0,0]) > 1.0e-6:\n",
    "        pts += [np.array([(-th_0 - y * th[1,0]) / th[0,0], y]) \\\n",
    "                                                         for y in (ymin, ymax)]\n",
    "    in_pts = []\n",
    "    for p in pts:\n",
    "        if (xmin-eps) <= p[0] <= (xmax+eps) and \\\n",
    "           (ymin-eps) <= p[1] <= (ymax+eps):\n",
    "            duplicate = False\n",
    "            for p1 in in_pts:\n",
    "                if np.max(np.abs(p - p1)) < 1.0e-6:\n",
    "                    duplicate = True\n",
    "            if not duplicate:\n",
    "                in_pts.append(p)\n",
    "    if in_pts and len(in_pts) >= 2:\n",
    "        # Plot separator\n",
    "        vpts = np.vstack(in_pts)\n",
    "        ax.plot(vpts[:,0], vpts[:,1], 'k-', lw=2)\n",
    "        # Plot normal\n",
    "        vmid = 0.5*(in_pts[0] + in_pts[1])\n",
    "        scale = np.sum(th*th)**0.5\n",
    "        diff = in_pts[0] - in_pts[1]\n",
    "        dist = max(xmax-xmin, ymax-ymin)\n",
    "        vnrm = vmid + (dist/10)*(th.T[0]/scale)\n",
    "        vpts = np.vstack([vmid, vnrm])\n",
    "        ax.plot(vpts[:,0], vpts[:,1], 'k-', lw=2)\n",
    "        # Try to keep limits from moving around\n",
    "        ax.set_xlim((xmin, xmax))\n",
    "        ax.set_ylim((ymin, ymax))\n",
    "    else:\n",
    "        print('Separator not in plot range')\n",
    "\n",
    "def plot_data(data, labels, ax = None, clear = False,\n",
    "                  xmin = None, xmax = None, ymin = None, ymax = None):\n",
    "    if ax is None:\n",
    "        if xmin == None: xmin = np.min(data[0, :]) - 0.5\n",
    "        if xmax == None: xmax = np.max(data[0, :]) + 0.5\n",
    "        if ymin == None: ymin = np.min(data[1, :]) - 0.5\n",
    "        if ymax == None: ymax = np.max(data[1, :]) + 0.5\n",
    "        ax = tidy_plot(xmin, xmax, ymin, ymax)\n",
    "\n",
    "        x_range = xmax - xmin; y_range = ymax - ymin\n",
    "        if .1 < x_range / y_range < 10:\n",
    "            ax.set_aspect('equal')\n",
    "        xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "    elif clear:\n",
    "        xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "        ax.clear()\n",
    "    else:\n",
    "        xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "    colors = np.choose(labels > 0, cv(['r', 'g']))[0]\n",
    "    ax.scatter(data[0,:], data[1,:], c = colors,\n",
    "                    marker = 'o', s=50, edgecolors = 'none')\n",
    "    # Seems to occasionally mess up the limits\n",
    "    ax.set_xlim(xlim); ax.set_ylim(ylim)\n",
    "    ax.grid(True, which='both')\n",
    "    #ax.axhline(y=0, color='k')\n",
    "    #ax.axvline(x=0, color='k')\n",
    "    return ax\n",
    "\n",
    "######################################################################\n",
    "#   Utilities\n",
    "\n",
    "# Takes a list of numbers and returns a column vector:  n x 1\n",
    "def cv(value_list):\n",
    "    return np.transpose(rv(value_list))\n",
    "\n",
    "# Takes a list of numbers and returns a row vector: 1 x n\n",
    "def rv(value_list):\n",
    "    return np.array([value_list])\n",
    "\n",
    "# x is dimension d by n\n",
    "# th is dimension d by m\n",
    "# th0 is dimension 1 by m\n",
    "# return matrix of y values for each column of x and theta: dimension m x n\n",
    "def y(x, th, th0):\n",
    "    return np.dot(np.transpose(th), x) + np.transpose(th0)\n",
    "\n",
    "def length(d_by_m):\n",
    "    return np.sum(d_by_m * d_by_m, axis = 0, keepdims = True)**0.5\n",
    "\n",
    "# x is dimension d by n\n",
    "# th is dimension d by m\n",
    "# th0 is dimension 1 by m\n",
    "# return matrix of signed dist for each column of x and theta: dimension m x n\n",
    "def signed_dist(x, th, th0):\n",
    "    return y(x, th, th0) / np.transpose(length(th))\n",
    "######################################################################\n",
    "# Perceptron code\n",
    "\n",
    "# data is dimension d by n\n",
    "# labels is dimension 1 by n\n",
    "# T is a positive integer number of steps to run\n",
    "# Perceptron algorithm with offset.\n",
    "# data is dimension d by n\n",
    "# labels is dimension 1 by n\n",
    "# T is a positive integer number of steps to run\n",
    "def perceptron(data, labels, params = {}, hook = None):\n",
    "    # if T not in params, default to 50\n",
    "    T = params.get('T', 50)\n",
    "    (d, n) = data.shape\n",
    "\n",
    "    theta = np.zeros((d, 1)); theta_0 = np.zeros((1, 1))\n",
    "    for t in range(T):\n",
    "        for i in range(n):\n",
    "            x = data[:,i:i+1]\n",
    "            y = labels[:,i:i+1]\n",
    "            if y * positive(x, theta, theta_0) <= 0.0:\n",
    "                theta = theta + y * x\n",
    "                theta_0 = theta_0 + y\n",
    "                if hook: hook((theta, theta_0))\n",
    "    return theta, theta_0\n",
    "\n",
    "def averaged_perceptron(data, labels, params = {}, hook = None):\n",
    "    T = params.get('T', 100)\n",
    "    (d, n) = data.shape\n",
    "\n",
    "    theta = np.zeros((d, 1)); theta_0 = np.zeros((1, 1))\n",
    "    theta_sum = theta.copy()\n",
    "    theta_0_sum = theta_0.copy()\n",
    "    for t in range(T):\n",
    "        for i in range(n):\n",
    "            x = data[:,i:i+1]\n",
    "            y = labels[:,i:i+1]\n",
    "            if y * positive(x, theta, theta_0) <= 0.0:\n",
    "                theta = theta + y * x\n",
    "                theta_0 = theta_0 + y\n",
    "                if hook: hook((theta, theta_0))\n",
    "            theta_sum = theta_sum + theta\n",
    "            theta_0_sum = theta_0_sum + theta_0\n",
    "    theta_avg = theta_sum / (T*n)\n",
    "    theta_0_avg = theta_0_sum / (T*n)\n",
    "    if hook: hook((theta_avg, theta_0_avg))\n",
    "    return theta_avg, theta_0_avg\n",
    "\n",
    "def positive(x, th, th0):\n",
    "    return np.sign(th.T@x + th0)\n",
    "\n",
    "def score(data, labels, th, th0):\n",
    "    return np.sum(positive(data, th, th0) == labels)\n",
    "\n",
    "def eval_classifier(learner, data_train, labels_train, data_test, labels_test):\n",
    "    th, th0 = learner(data_train, labels_train)\n",
    "    return score(data_test, labels_test, th, th0)/data_test.shape[1]\n",
    "\n",
    "def xval_learning_alg(learner, data, labels, k):\n",
    "    _, n = data.shape\n",
    "    idx = list(range(n))\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(idx)\n",
    "    data, labels = data[:,idx], labels[:,idx]\n",
    "\n",
    "    s_data = np.array_split(data, k, axis=1)\n",
    "    s_labels = np.array_split(labels, k, axis=1)\n",
    "\n",
    "    score_sum = 0\n",
    "    for i in range(k):\n",
    "        data_train = np.concatenate(s_data[:i] + s_data[i+1:], axis=1)\n",
    "        labels_train = np.concatenate(s_labels[:i] + s_labels[i+1:], axis=1)\n",
    "        data_test = np.array(s_data[i])\n",
    "        labels_test = np.array(s_labels[i])\n",
    "        score_sum += eval_classifier(learner, data_train, labels_train,\n",
    "                                              data_test, labels_test)\n",
    "    return score_sum/k\n",
    "\n",
    "######################################################################\n",
    "#   Tests\n",
    "\n",
    "def test_linear_classifier(dataFun, learner, learner_params = {},\n",
    "                             draw = True, refresh = True, pause = True):\n",
    "    data, labels = dataFun()\n",
    "    d, n = data.shape\n",
    "    if draw:\n",
    "        ax = plot_data(data, labels)\n",
    "        def hook(params):\n",
    "            (th, th0) = params\n",
    "            if refresh: plot_data(data, labels, ax, clear = True)\n",
    "            plot_separator(ax, th, th0)\n",
    "            print('th', th.T, 'th0', th0)\n",
    "            if pause: input('go?')\n",
    "    else:\n",
    "        hook = None\n",
    "    th, th0 = learner(data, labels, hook = hook, params = learner_params)\n",
    "    print(\"Final score\", float(score(data, labels, th, th0)) / n)\n",
    "    print(\"Params\", np.transpose(th), th0)\n",
    "\n",
    "######################################################################\n",
    "# For auto dataset\n",
    "\n",
    "def load_auto_data(path_data):\n",
    "    \"\"\"\n",
    "    Returns a list of dict with keys:\n",
    "    \"\"\"\n",
    "    numeric_fields = {'mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "                      'acceleration', 'model_year', 'origin'}\n",
    "    data = []\n",
    "    with open(path_data) as f_data:\n",
    "        for datum in csv.DictReader(f_data, delimiter='\\t'):\n",
    "            for field in list(datum.keys()):\n",
    "                if field in numeric_fields and datum[field]:\n",
    "                    datum[field] = float(datum[field])\n",
    "            data.append(datum)\n",
    "    return data\n",
    "\n",
    "# Feature transformations\n",
    "def std_vals(data, f):\n",
    "    vals = [entry[f] for entry in data]\n",
    "    avg = sum(vals)/len(vals)\n",
    "    dev = [(entry[f] - avg)**2 for entry in data]\n",
    "    sd = (sum(dev)/len(vals))**0.5\n",
    "    return (avg, sd)\n",
    "\n",
    "def standard(v, std):\n",
    "    return [(v-std[0])/std[1]]\n",
    "\n",
    "def raw(x):\n",
    "    return [x]\n",
    "\n",
    "def one_hot(v, entries):\n",
    "    vec = len(entries)*[0]\n",
    "    vec[entries.index(v)] = 1\n",
    "    return vec\n",
    "\n",
    "# The class (mpg) added to the front of features\n",
    "def auto_data_and_labels(auto_data, features):\n",
    "    features = [('mpg', raw)] + features\n",
    "    std = {f:std_vals(auto_data, f) for (f, phi) in features if phi==standard}\n",
    "    entries = {f:list(set([entry[f] for entry in auto_data])) \\\n",
    "               for (f, phi) in features if phi==one_hot}\n",
    "    print('avg and std', std)\n",
    "    print('entries in one_hot field', entries)\n",
    "    vals = []\n",
    "    for entry in auto_data:\n",
    "        phis = []\n",
    "        for (f, phi) in features:\n",
    "            if phi == standard:\n",
    "                phis.extend(phi(entry[f], std[f]))\n",
    "            elif phi == one_hot:\n",
    "                phis.extend(phi(entry[f], entries[f]))\n",
    "            else:\n",
    "                phis.extend(phi(entry[f]))\n",
    "        vals.append(np.array([phis]))\n",
    "    data_labels = np.vstack(vals)\n",
    "    return data_labels[:, 1:].T, data_labels[:, 0:1].T\n",
    "\n",
    "######################################################################\n",
    "# For food review dataset\n",
    "\n",
    "from string import punctuation, digits, printable\n",
    "import csv\n",
    "\n",
    "def load_review_data(path_data):\n",
    "    \"\"\"\n",
    "    Returns a list of dict with keys:\n",
    "    * sentiment: +1 or -1 if the review was positive or negative, respectively\n",
    "    * text: the text of the review\n",
    "    \"\"\"\n",
    "    basic_fields = {'sentiment', 'text'}\n",
    "    data = []\n",
    "    with open(path_data) as f_data:\n",
    "        for datum in csv.DictReader(f_data, delimiter='\\t'):\n",
    "            for field in list(datum.keys()):\n",
    "                if field not in basic_fields:\n",
    "                    del datum[field]\n",
    "            if datum['sentiment']:\n",
    "                datum['sentiment'] = int(datum['sentiment'])\n",
    "            data.append(datum)\n",
    "    return data\n",
    "\n",
    "printable = set(printable)\n",
    "def clean(s):\n",
    "    return filter(lambda x: x in printable, s)\n",
    "\n",
    "def extract_words(input_string):\n",
    "    \"\"\"\n",
    "    Helper function for bag_of_words()\n",
    "    Inputs a text string\n",
    "    Returns a list of lowercase words in the string.\n",
    "    Punctuation and digits are separated out into their own words.\n",
    "    \"\"\"\n",
    "    for c in punctuation + digits:\n",
    "        input_string = input_string.replace(c, ' ' + c + ' ')\n",
    "\n",
    "    # return [ps.stem(w) for w in input_string.lower().split()]\n",
    "    return input_string.lower().split()\n",
    "\n",
    "def bag_of_words(texts):\n",
    "    \"\"\"\n",
    "    Inputs a list of string reviews\n",
    "    Returns a dictionary of unique unigrams occurring over the input\n",
    "\n",
    "    Feel free to change this code as guided by Section 3 (e.g. remove stopwords, add bigrams etc.)\n",
    "    \"\"\"\n",
    "    dictionary = {} # maps word to unique index\n",
    "    for text in texts:\n",
    "        word_list = extract_words(text)\n",
    "        for word in word_list:\n",
    "            if word not in dictionary:\n",
    "                dictionary[word] = len(dictionary)\n",
    "    return dictionary\n",
    "\n",
    "def extract_bow_feature_vectors(reviews, dictionary):\n",
    "    \"\"\"\n",
    "    Inputs a list of string reviews\n",
    "    Inputs the dictionary of words as given by bag_of_words\n",
    "    Returns the bag-of-words feature matrix representation of the data.\n",
    "    The returned matrix is of shape (n, m), where n is the number of reviews\n",
    "    and m the total number of entries in the dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    num_reviews = len(reviews)\n",
    "    feature_matrix = np.zeros([num_reviews, len(dictionary)])\n",
    "\n",
    "    for i, text in enumerate(reviews):\n",
    "        word_list = extract_words(text)\n",
    "        for word in word_list:\n",
    "            if word in dictionary:\n",
    "                feature_matrix[i, dictionary[word]] = 1\n",
    "    # We want the feature vectors as columns\n",
    "    return feature_matrix.T\n",
    "\n",
    "\n",
    "def reverse_dict(d):\n",
    "    return {v: k for k, v in d.items()}\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# For MNIST dataset\n",
    "\n",
    "\n",
    "# NOTE you should use this function to evaluate your MNIST results!\n",
    "def get_classification_accuracy(data, labels):\n",
    "    \"\"\"\n",
    "    @param data (d,n) array\n",
    "    @param labels (1,n) array\n",
    "    \"\"\"\n",
    "    return xval_learning_alg(lambda data, labels: perceptron(data, labels, {\"T\": 50}), data, labels, 10)\n",
    "\n",
    "\n",
    "def load_mnist_data(labels):\n",
    "    \"\"\"\n",
    "    @param labels list of labels from {0, 1,...,9}\n",
    "    @return dict: label (int) -> [[image1], [image2], ...]\n",
    "    \"\"\"\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for label in labels:\n",
    "        images = load_mnist_single(\"mnist/mnist_train{}.png\".format(label))\n",
    "        y = np.array([[label] * len(images)])\n",
    "        data[label] = {\n",
    "            \"images\": images,\n",
    "            \"labels\": y\n",
    "        }\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_mnist_single(path_data):\n",
    "    \"\"\"\n",
    "    @return list of images (first row of large picture)\n",
    "    \"\"\"\n",
    "\n",
    "    img = imread(path_data)  # 2156 x 2156 (m,n)\n",
    "    m, n = img.shape\n",
    "\n",
    "    side_len = 28  # standard mnist\n",
    "    n_img = int(m / 28)\n",
    "\n",
    "    imgs = []  # list of single images\n",
    "    for i in range(n_img):\n",
    "        start_ind = i*side_len\n",
    "        end_ind = start_ind + side_len\n",
    "        current_img = img[start_ind:end_ind, :side_len]  # 28 by 28\n",
    "\n",
    "        current_img = current_img / 255 # normalization!!!\n",
    "\n",
    "        imgs.append(current_img)\n",
    "\n",
    "    return imgs\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "print(\"Imported tidy_plot, plot_separator, plot_data, plot_nonlin_sep, cv, rv, y, positive, score\")\n",
    "print(\"         xval_learning_alg, eval_classifier\")\n",
    "print(\"Tests: test_linear_classifier\")\n",
    "print(\"Dataset tools: load_auto_data, std_vals, standard, raw, one_hot, auto_data_and_labels\")\n",
    "print(\"               load_review_data, clean, extract_words, bag_of_words, extract_bow_feature_vectors\")\n",
    "print(\"               load_mnist_data, load_mnist_single\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg and std {}\n",
      "entries in one_hot field {}\n",
      "(array([[193., 200., 215., 210., 180., 150., 208., 160., 180., 180., 167.,\n",
      "        198., 225., 129., 130., 140., 130., 150., 150., 145., 145., 150.,\n",
      "        155., 165., 175., 158., 170., 175., 150., 170., 175., 190., 215.,\n",
      "        137., 140., 150., 150., 150., 150., 150., 150., 160., 165., 148.,\n",
      "        153., 153., 175., 175., 215., 220., 225., 225., 152., 100., 720.,\n",
      "        720., 110., 130., 150., 150., 150., 150., 145., 145., 165., 170.,\n",
      "        190., 150., 198., 120., 145., 170., 142., 190., 105., 100., 100.,\n",
      "        105., 110., 140., 150., 150., 150., 149., 170., 180., 230., 133.,\n",
      "        120., 180., 138., 155., 125., 110., 100., 110., 140., 150., 130.,\n",
      "        110., 950., 140., 145., 140., 850., 129., 165., 900., 112., 970.,\n",
      "        970., 105., 105., 950., 100., 100., 100., 105., 780., 880., 880.,\n",
      "        110., 130., 150., 120., 139., 135., 110., 980., 150., 110., 970.,\n",
      "        880., 112., 850., 108., 100., 950., 100., 100., 900., 100., 880.,\n",
      "        900., 105., 125., 145., 900., 140., 850., 110., 910., 102., 900.,\n",
      "        880., 122., 950., 100., 100., 110., 850., 880., 900., 139., 103.,\n",
      "        950., 100., 105., 110., 105., 850., 870., 860., 720., 107., 900.,\n",
      "        850., 110., 950., 110., 110., 115., 115., 940., 760., 980., 860.,\n",
      "        720., 970., 950., 100., 112., 105., 880., 110., 900., 950., 880.,\n",
      "        970., 860., 780., 830., 540., 950., 125., 105., 110., 100., 850.,\n",
      "        970., 900., 900., 950., 950., 750., 970., 970., 110., 960., 920.,\n",
      "        750., 810., 120., 900., 880., 600., 950., 870., 950., 810., 115.,\n",
      "        750., 920., 710., 800., 110., 880., 770., 116., 960., 890., 920.,\n",
      "        930., 750., 113., 800., 920., 670., 700., 690., 460., 460., 750.,\n",
      "        780., 790., 900., 880., 720., 840., 105., 115., 830., 880., 860.,\n",
      "        900., 900., 600., 880., 880., 880., 970., 840., 710., 800., 950.,\n",
      "        105., 860., 880., 900., 790., 900., 900., 750., 750., 920., 800.,\n",
      "        800., 900., 115., 840., 490., 520., 700., 700., 750., 780., 830.,\n",
      "        710., 680., 900., 620., 650., 800., 840., 670., 700., 760., 670.,\n",
      "        680., 780., 630., 760., 750., 850., 820., 650., 520., 670., 670.,\n",
      "        680., 750., 710., 680., 740., 650., 710., 840., 960., 650., 610.,\n",
      "        700., 670., 700., 750., 670., 720., 750., 132., 520., 100., 740.,\n",
      "        530., 530., 900., 700., 830., 750., 670., 700., 880., 650., 680.,\n",
      "        700., 780., 650., 700., 630., 880., 690., 600., 800., 740., 750.,\n",
      "        880., 840., 580., 700., 600., 660., 670., 920., 650., 680., 650.,\n",
      "        690., 620., 630., 670., 670., 850., 600., 640., 580., 700., 650.,\n",
      "        760., 480., 480., 520., 480., 670., 650.]]), array([[-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1.,  1.]]))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "one_hot() missing 2 required positional arguments: 'v' and 'entries'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m load_auto_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto-mpg.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(auto_data_and_labels(data, [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhorsepower\u001b[39m\u001b[38;5;124m'\u001b[39m, raw), ]))\n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m one_hot()\n",
      "\u001b[0;31mTypeError\u001b[0m: one_hot() missing 2 required positional arguments: 'v' and 'entries'"
     ]
    }
   ],
   "source": [
    "data = load_auto_data('auto-mpg.tsv')\n",
    "print(auto_data_and_labels(data, [('horsepower', raw), ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg and std {}\n",
      "entries in one_hot field {'cylinders': [3.0, 4.0, 5.0, 6.0, 8.0], 'origin': [1.0, 2.0, 3.0]}\n"
     ]
    }
   ],
   "source": [
    "dataf, layers = auto_data_and_labels(data, [('cylinders', one_hot), ('displacement', raw), ('horsepower', raw), ('weight', raw), ('acceleration', raw), ('model_year', raw), ('origin', one_hot)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7012820512820512\n"
     ]
    }
   ],
   "source": [
    "perceptron(dataf, layers, params = {'T': 1}, hook = None)\n",
    "print(xval_learning_alg(perceptron, dataf, layers, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg and std {}\n",
      "entries in one_hot field {}\n",
      "avg and std {'displacement': (388.3482142857143, 302.0458123396403), 'horsepower': (509.3545918367347, 333.6521151716361), 'weight': (2977.5841836734694, 848.3184465698365), 'acceleration': (15.541326530612228, 2.7553429127509963)}\n",
      "entries in one_hot field {'cylinders': [3.0, 4.0, 5.0, 6.0, 8.0], 'origin': [1.0, 2.0, 3.0]}\n",
      "Features: [('cylinders', <function raw at 0x12726c2c0>), ('displacement', <function raw at 0x12726c2c0>), ('horsepower', <function raw at 0x12726c2c0>), ('weight', <function raw at 0x12726c2c0>), ('acceleration', <function raw at 0x12726c2c0>), ('origin', <function raw at 0x12726c2c0>)], Algorithm: perceptron, T: 1, Accuracy: 0.6526\n",
      "Features: [('cylinders', <function raw at 0x12726c2c0>), ('displacement', <function raw at 0x12726c2c0>), ('horsepower', <function raw at 0x12726c2c0>), ('weight', <function raw at 0x12726c2c0>), ('acceleration', <function raw at 0x12726c2c0>), ('origin', <function raw at 0x12726c2c0>)], Algorithm: perceptron, T: 10, Accuracy: 0.7423\n",
      "Features: [('cylinders', <function raw at 0x12726c2c0>), ('displacement', <function raw at 0x12726c2c0>), ('horsepower', <function raw at 0x12726c2c0>), ('weight', <function raw at 0x12726c2c0>), ('acceleration', <function raw at 0x12726c2c0>), ('origin', <function raw at 0x12726c2c0>)], Algorithm: perceptron, T: 50, Accuracy: 0.6910\n",
      "Features: [('cylinders', <function raw at 0x12726c2c0>), ('displacement', <function raw at 0x12726c2c0>), ('horsepower', <function raw at 0x12726c2c0>), ('weight', <function raw at 0x12726c2c0>), ('acceleration', <function raw at 0x12726c2c0>), ('origin', <function raw at 0x12726c2c0>)], Algorithm: averaged_perceptron, T: 1, Accuracy: 0.8441\n",
      "Features: [('cylinders', <function raw at 0x12726c2c0>), ('displacement', <function raw at 0x12726c2c0>), ('horsepower', <function raw at 0x12726c2c0>), ('weight', <function raw at 0x12726c2c0>), ('acceleration', <function raw at 0x12726c2c0>), ('origin', <function raw at 0x12726c2c0>)], Algorithm: averaged_perceptron, T: 10, Accuracy: 0.8366\n",
      "Features: [('cylinders', <function raw at 0x12726c2c0>), ('displacement', <function raw at 0x12726c2c0>), ('horsepower', <function raw at 0x12726c2c0>), ('weight', <function raw at 0x12726c2c0>), ('acceleration', <function raw at 0x12726c2c0>), ('origin', <function raw at 0x12726c2c0>)], Algorithm: averaged_perceptron, T: 50, Accuracy: 0.8366\n",
      "Features: [('cylinders', <function one_hot at 0x1076fbce0>), ('displacement', <function standard at 0x1076fbc40>), ('horsepower', <function standard at 0x1076fbc40>), ('weight', <function standard at 0x1076fbc40>), ('acceleration', <function standard at 0x1076fbc40>), ('origin', <function one_hot at 0x1076fbce0>)], Algorithm: perceptron, T: 1, Accuracy: 0.7908\n",
      "Features: [('cylinders', <function one_hot at 0x1076fbce0>), ('displacement', <function standard at 0x1076fbc40>), ('horsepower', <function standard at 0x1076fbc40>), ('weight', <function standard at 0x1076fbc40>), ('acceleration', <function standard at 0x1076fbc40>), ('origin', <function one_hot at 0x1076fbce0>)], Algorithm: perceptron, T: 10, Accuracy: 0.8062\n",
      "Features: [('cylinders', <function one_hot at 0x1076fbce0>), ('displacement', <function standard at 0x1076fbc40>), ('horsepower', <function standard at 0x1076fbc40>), ('weight', <function standard at 0x1076fbc40>), ('acceleration', <function standard at 0x1076fbc40>), ('origin', <function one_hot at 0x1076fbce0>)], Algorithm: perceptron, T: 50, Accuracy: 0.8060\n",
      "Features: [('cylinders', <function one_hot at 0x1076fbce0>), ('displacement', <function standard at 0x1076fbc40>), ('horsepower', <function standard at 0x1076fbc40>), ('weight', <function standard at 0x1076fbc40>), ('acceleration', <function standard at 0x1076fbc40>), ('origin', <function one_hot at 0x1076fbce0>)], Algorithm: averaged_perceptron, T: 1, Accuracy: 0.9004\n",
      "Features: [('cylinders', <function one_hot at 0x1076fbce0>), ('displacement', <function standard at 0x1076fbc40>), ('horsepower', <function standard at 0x1076fbc40>), ('weight', <function standard at 0x1076fbc40>), ('acceleration', <function standard at 0x1076fbc40>), ('origin', <function one_hot at 0x1076fbce0>)], Algorithm: averaged_perceptron, T: 10, Accuracy: 0.8979\n",
      "Features: [('cylinders', <function one_hot at 0x1076fbce0>), ('displacement', <function standard at 0x1076fbc40>), ('horsepower', <function standard at 0x1076fbc40>), ('weight', <function standard at 0x1076fbc40>), ('acceleration', <function standard at 0x1076fbc40>), ('origin', <function one_hot at 0x1076fbce0>)], Algorithm: averaged_perceptron, T: 50, Accuracy: 0.9005\n"
     ]
    }
   ],
   "source": [
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg and std {'displacement': (388.3482142857143, 302.0458123396403), 'horsepower': (509.3545918367347, 333.6521151716361), 'weight': (2977.5841836734694, 848.3184465698365), 'acceleration': (15.541326530612228, 2.7553429127509963), 'model_year': (75.9795918367347, 3.679034899615172)}\n",
      "entries in one_hot field {'cylinders': [3.0, 4.0, 5.0, 6.0, 8.0], 'origin': [1.0, 2.0, 3.0]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-1.98545918],\n",
       "        [ 4.32066327],\n",
       "        [-0.75816327],\n",
       "        [-1.41989796],\n",
       "        [ 0.18443878],\n",
       "        [ 0.42630439],\n",
       "        [ 0.9559673 ],\n",
       "        [-4.53494002],\n",
       "        [-0.16294281],\n",
       "        [ 3.95541888],\n",
       "        [ 0.78596939],\n",
       "        [ 0.68214286],\n",
       "        [-1.12653061]]),\n",
       " array([[0.34158163]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datass, labelss = auto_data_and_labels(auto_data, [('cylinders', one_hot), ('displacement', standard), ('horsepower', standard), ('weight', standard), ('acceleration', standard), ('model_year', standard), ('origin', one_hot)])\n",
    "averaged_perceptron(datass, labelss, params = {'T': 10}, hook = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
