{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xIaEwCD406A"
      },
      "source": [
        "#MIT 6.036 Spring 2019: Homework 8#\n",
        "\n",
        "This colab notebook provides code and a framework for [homework 8](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week8/week8_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, download the code distribution for this homework that contains test cases and helper functions.\n",
        "\n",
        "Run the next code block to download and import the code for this lab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-sSs7N4mMiX"
      },
      "source": [
        "# 2) Implementing Mini-batch Gradient Descent and Batch Normalization (OPTIONAL)\n",
        "\n",
        "** Note: You can click the arrow on the left of this text block to collapse/expand this optional section and all its code blocks **\n",
        "\n",
        "Last week we implemented a framework for building neural networks from scratch. We trained our models using *stochastic* gradient descent. In this problem, we explore how we can implement batch normalization as a module `BatchNorm` in our framework. It is the same module which you analyzed in problem 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgxmIfXVmVwd"
      },
      "source": [
        "Key to the concept of batch normalization is the doing gradient descent on batches of data. So we instead of using last week's stochastic gradient descent, we will first implement the *mini-batch* gradient descent method `mini_gd`, which is a hybrid between *stochastic* gradient descent and *batch* gradient descent. The lecture notes on <a href=\"https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week7/neural_networks_2/1?activate_block_id=block-v1%3AMITx%2B6.036%2B2019_Spring%2Btype%40vertical%2Bblock%40neural_networks_2_optimizing_neural_network_parameters_vert\"> optimizing neural network parameters</a> are helpful for this part.\n",
        "\n",
        "In *mini-batch* gradient descent, for a mini-batch of size $K$, we select $K$ distinct data points uniformly at random from the data set and update the network weights based only on their contributions to the gradient:\n",
        "$$W := W - \\eta\\sum_{i=1}^K \\nabla_W \\mathcal{L}(h(x^{(i)}; W), y^{(i)})\\;\\;.$$\n",
        "\n",
        "Our *mini-batch* method `mini_gd` will be implemented within the `Sequential` python class (see homework 7 problem 2) and will take the following as inputs:\n",
        "\n",
        "* `X`: a standard data array (d by n)\n",
        "* `y`: a standard labels row vector (1 by n)\n",
        "* `iters`: the number of updates to perform on weights $W$\n",
        "* `lrate`: the learning rate used\n",
        "* `K`: the mini-batch size to be used\n",
        "\n",
        "One call of `mini_gd` should call `Sequential.backward` for back-propagation and `Sequential.step` for updating the weights, for a total of `iters` times, using `lrate` as the learning rate. As in our implementation of `sgd` from homework 7, we compute the predicted output for a mini-batch of data with the `Sequential.forward` method. We compute the loss between our predictions and the true labels using the assigned `Sequential.loss` method. (Note that in homework 7, `Sequential.step` was called `Sequential.sgd_step`. While the functionality of the step function is the same, it has been renamed for convenience. The same is true for the `module.step` function of each module we implemented, where applicable.)\n",
        "\n",
        "For picking $K$ unique data points at random from our large data-set for each mini-batch, we will implement the following strategy: we will first shuffle our data points `X` (and associated labels `y`). Then, we get <math>\\frac{n}{k}</math> (rounded down to the nearest integer) different mini-batches by grouping each $K$ consecutive points from this shuffled array. If we end up iterating over all the points but need more mini-batches, we will repeat the shuffling and the batching process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr1kWI08mdo4"
      },
      "source": [
        "<b>2A)</b>You need to fill in the missing code below. We have implemented the shuffling of indices and have provided you with the outer and inner loops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_lvmO9Z22bH"
      },
      "source": [
        "** This OPTIONAL problem has you extend your homework 7 implementation for building neural networks. **\n",
        "### PLEASE COPY IN YOUR CODE FROM HOMEWORK 7 TO COMPLEMENT THE CLASSES GIVEN HERE\n",
        "\n",
        "Recall that your implementation from homework 7 included the following classes:\n",
        "    \n",
        "  * Module\n",
        "  * Linear\n",
        "  * Tanh\n",
        "  * ReLU\n",
        "  * SoftMax\n",
        "  * NLL\n",
        "  * Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'.conda (Python 3.12.0)' ile hücreleri çalıştırmak için ipykernel paketi gerekir.\n",
            "\u001b[1;31mPython ortamına 'ipykernel' yüklemek için aşağıdaki komutu çalıştırın. \n",
            "\u001b[1;31mKomut: 'conda install -p /Users/hbasriuzun/Documents/GitHub/RubicLAB/.conda ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# This code file pertains to problems 3 onwards from homework 8\n",
        "# Here, we use out-of-the-box neural network frameworks Keras and Tensorflow \n",
        "# to build and train our models\n",
        "\n",
        "import pdb\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "import math as m \n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.layers import Conv1D, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.initializers import VarianceScaling\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Problem 3 - 2D data\n",
        "######################################################################\n",
        "\n",
        "def archs(classes):\n",
        "    return [[Dense(input_dim=2, units=classes, activation=\"softmax\")],\n",
        "            [Dense(input_dim=2, units=10, activation='relu'),\n",
        "             Dense(units=classes, activation=\"softmax\")],\n",
        "            [Dense(input_dim=2, units=100, activation='relu'),\n",
        "             Dense(units=classes, activation=\"softmax\")],\n",
        "            [Dense(input_dim=2, units=10, activation='relu'),\n",
        "             Dense(units=10, activation='relu'),\n",
        "             Dense(units=classes, activation=\"softmax\")],\n",
        "            [Dense(input_dim=2, units=100, activation='relu'),\n",
        "             Dense(units=100, activation='relu'),\n",
        "             Dense(units=classes, activation=\"softmax\")]]\n",
        "\n",
        "# Read the simple 2D dataset files\n",
        "def get_data_set(name):\n",
        "    try:\n",
        "        data = np.loadtxt(name, skiprows=0, delimiter = ' ')\n",
        "    except:\n",
        "        return None, None, None\n",
        "    np.random.shuffle(data)             # shuffle the data\n",
        "    # The data uses ROW vectors for a data point, that's what Keras assumes.\n",
        "    _, d = data.shape\n",
        "    X = data[:,0:d-1]\n",
        "    Y = data[:,d-1:d]\n",
        "    y = Y.T[0]\n",
        "    classes = set(y)\n",
        "    if classes == set([-1.0, 1.0]):\n",
        "        print('Convert from -1,1 to 0,1')\n",
        "        y = 0.5*(y+1)\n",
        "    print('Loading X', X.shape, 'y', y.shape, 'classes', set(y))\n",
        "    return X, y, len(classes)\n",
        "\n",
        "######################################################################\n",
        "# General helpers for Problems 3-5\n",
        "######################################################################\n",
        "\n",
        "class LossHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.keys = ['loss', 'acc', 'val_loss', 'val_acc']\n",
        "        self.values = {}\n",
        "        for k in self.keys:\n",
        "            self.values['batch_'+k] = []\n",
        "            self.values['epoch_'+k] = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        for k in self.keys:\n",
        "            bk = 'batch_'+k\n",
        "            if k in logs:\n",
        "                self.values[bk].append(logs[k])\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        for k in self.keys:\n",
        "            ek = 'epoch_'+k\n",
        "            if k in logs:\n",
        "                self.values[ek].append(logs[k])\n",
        "\n",
        "    def plot(self, keys):\n",
        "        for key in keys:\n",
        "            plt.plot(np.arange(len(self.values[key])), np.array(self.values[key]), label=key)\n",
        "        plt.legend()\n",
        "\n",
        "def run_keras(X_train, y_train, X_val, y_val, X_test, y_test, layers, epochs, split=0, verbose=True):\n",
        "    # Model specification\n",
        "    model = Sequential()\n",
        "    for layer in layers:\n",
        "        model.add(layer)\n",
        "    # Define the optimization\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=[\"accuracy\"])\n",
        "    N = X_train.shape[0]\n",
        "    # Pick batch size\n",
        "    batch = 32 if N > 1000 else 1     # batch size\n",
        "    history = LossHistory()\n",
        "    # Fit the model\n",
        "    if X_val is None:\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch, validation_split=split,\n",
        "                  callbacks=[history], verbose=verbose)\n",
        "    else:\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch, validation_data=(X_val, y_val),\n",
        "                  callbacks=[history], verbose=verbose)\n",
        "    # Evaluate the model on validation data, if any\n",
        "    if X_val is not None or split > 0:\n",
        "        val_acc, val_loss = history.values['epoch_val_acc'][-1], history.values['epoch_val_loss'][-1]\n",
        "        print (\"\\nLoss on validation set:\"  + str(val_loss) + \" Accuracy on validation set: \" + str(val_acc))\n",
        "    else:\n",
        "        val_acc = None\n",
        "    # Evaluate the model on test data, if any\n",
        "    if X_test is not None:\n",
        "        test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=batch)\n",
        "        print (\"\\nLoss on test set:\"  + str(test_loss) + \" Accuracy on test set: \" + str(test_acc))\n",
        "    else:\n",
        "        test_acc = None\n",
        "    return model, history, val_acc, test_acc\n",
        "\n",
        "def dataset_paths(data_name):\n",
        "    return [\"data/data\"+data_name+\"_\"+suffix+\".csv\" for suffix in (\"train\", \"validate\", \"test\")]\n",
        "\n",
        "# The name is a string such as \"1\" or \"Xor\"\n",
        "def run_keras_2d(data_name, layers, epochs, display=True, split=0.25, verbose=True, trials=1):\n",
        "    print('Keras FC: dataset=', data_name)\n",
        "    (train_dataset, val_dataset, test_dataset) = dataset_paths(data_name)\n",
        "    # Load the datasets\n",
        "    X_train, y, num_classes = get_data_set(train_dataset)\n",
        "    X_val, y2, _ = get_data_set(val_dataset)\n",
        "    X_test, y3, _ = get_data_set(test_dataset)\n",
        "    # Categorize the labels\n",
        "    y_train = np_utils.to_categorical(y, num_classes) # one-hot\n",
        "    y_val = y_test = None\n",
        "    if X_val is not None:\n",
        "        y_val = np_utils.to_categorical(y2, num_classes) # one-hot        \n",
        "    if X_test is not None:\n",
        "        y_test = np_utils.to_categorical(y3, num_classes) # one-hot\n",
        "    val_acc, test_acc = 0, 0\n",
        "    for trial in range(trials):\n",
        "        # Reset the weights\n",
        "        # See https://github.com/keras-team/keras/issues/341\n",
        "        session = K.get_session()\n",
        "        for layer in layers:\n",
        "            for v in layer.__dict__:\n",
        "                v_arg = getattr(layer, v)\n",
        "                if hasattr(v_arg, 'initializer'):\n",
        "                    initializer_func = getattr(v_arg, 'initializer')\n",
        "                    initializer_func.run(session=session)\n",
        "        # Run the model\n",
        "        model, history, vacc, tacc, = \\\n",
        "               run_keras(X_train, y_train, X_val, y_val, X_test, y_test, layers, epochs,\n",
        "                         split=split, verbose=verbose)\n",
        "        val_acc += vacc if vacc else 0\n",
        "        test_acc += tacc if tacc else 0\n",
        "        if display:\n",
        "            # plot classifier landscape on training data\n",
        "            plot_heat(X_train, y, model)\n",
        "            plt.title('Training data')\n",
        "            plt.show()\n",
        "            if X_test is not None:\n",
        "                # plot classifier landscape on testing data\n",
        "                plot_heat(X_test, y3, model)\n",
        "                plt.title('Testing data')\n",
        "                plt.show()\n",
        "            # Plot epoch loss\n",
        "            history.plot(['epoch_loss', 'epoch_val_loss'])\n",
        "            plt.xlabel('epoch')\n",
        "            plt.ylabel('loss')\n",
        "            plt.title('Epoch val_loss and loss')\n",
        "            plt.show()\n",
        "            # Plot epoch accuracy\n",
        "            history.plot(['epoch_acc', 'epoch_val_acc'])\n",
        "            plt.xlabel('epoch')\n",
        "            plt.ylabel('accuracy')\n",
        "            plt.title('Epoch val_acc and acc')\n",
        "            plt.show()\n",
        "    if val_acc:\n",
        "        print (\"\\nAvg. validation accuracy:\"  + str(val_acc/trials))\n",
        "    if test_acc:\n",
        "        print (\"\\nAvg. test accuracy:\"  + str(test_acc/trials))\n",
        "    return X_train, y, model\n",
        "\n",
        "######################################################################\n",
        "# Helper functions for \n",
        "# OPTIONAL: Problem 4 - Weight Sharing\n",
        "######################################################################\n",
        "\n",
        "def generate_1d_images(nsamples,image_size,prob):\n",
        "    Xs=[]\n",
        "    Ys=[]\n",
        "    for i in range(0,nsamples):\n",
        "        X=np.random.binomial(1, prob, size=image_size)\n",
        "        Y=count_objects_1d(X)\n",
        "        Xs.append(X)\n",
        "        Ys.append(Y)\n",
        "    Xs=np.array(Xs)\n",
        "    Ys=np.array(Ys)\n",
        "    return Xs,Ys\n",
        "\n",
        "\n",
        "#count the number of objects in a 1d array\n",
        "def count_objects_1d(array):\n",
        "    count=0\n",
        "    for i in range(len(array)):\n",
        "        num=array[i]\n",
        "        if num==0:\n",
        "            if i==0 or array[i-1]==1:\n",
        "                count+=1\n",
        "    return count\n",
        "\n",
        "def l1_reg(weight_matrix):\n",
        "    return 0.01 * K.sum(K.abs(weight_matrix))    \n",
        "\n",
        "\n",
        "def filter_reg(weights):\n",
        "    lam=0\n",
        "    return lam* val\n",
        "\n",
        "def get_image_data_1d(tsize,image_size,prob):\n",
        "    #prob controls the density of white pixels\n",
        "    #tsize is the size of the training and test sets\n",
        "    vsize=int(0.2*tsize)\n",
        "    X_train,Y_train=generate_1d_images(tsize,image_size,prob)\n",
        "    X_val,Y_val=generate_1d_images(vsize,image_size,prob)\n",
        "    X_test,Y_test=generate_1d_images(tsize,image_size,prob)\n",
        "    #reshape the input data for the convolutional layer\n",
        "    X_train=np.expand_dims(X_train,axis=2)\n",
        "    X_val=np.expand_dims(X_val,axis=2)\n",
        "    X_test=np.expand_dims(X_test,axis=2)\n",
        "    data=(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
        "    return data\n",
        "\n",
        "def train_neural_counter(layers,data,loss_func='mse',display=False):\n",
        "    (X_train,Y_train,X_val,Y_val,X_test,Y_test)=data\n",
        "    epochs=10\n",
        "    batch=1\n",
        "    \n",
        "    model=Sequential()\n",
        "    for layer in layers:\n",
        "        model.add(layer)\n",
        "    model.summary()    \n",
        "    model.compile(loss=loss_func, optimizer=Adam())\n",
        "    history = LossHistory()    \n",
        "    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch, validation_data=(X_val, Y_val),callbacks=[history], verbose=True)\n",
        "    err=model.evaluate(X_test,Y_test)\n",
        "    ws=model.layers[-1].get_weights()[0]\n",
        "    if display:\n",
        "        plt.plot(ws)\n",
        "        plt.show()\n",
        "    return model,err\n",
        "\n",
        "######################################################################\n",
        "# Problem 5\n",
        "######################################################################\n",
        "\n",
        "def shifted(X, shift):\n",
        "    n = X.shape[0]\n",
        "    m = X.shape[1]\n",
        "    size = m + shift\n",
        "    X_sh = np.zeros((n, size, size))\n",
        "    plt.ion()\n",
        "    for i in range(n):\n",
        "        sh1 = np.random.randint(shift)\n",
        "        sh2 = np.random.randint(shift)\n",
        "        X_sh[i, sh1:sh1+m, sh2:sh2+m] = X[i, :, :]\n",
        "        # If you want to see the shifts, uncomment\n",
        "        #plt.figure(1); plt.imshow(X[i])\n",
        "        #plt.figure(2); plt.imshow(X_sh[i])\n",
        "        #plt.show()\n",
        "        #input('Go?')\n",
        "    return X_sh\n",
        "  \n",
        "def get_MNIST_data(shift=0):\n",
        "    (X_train, y1), (X_val, y2) = mnist.load_data()\n",
        "    if shift:\n",
        "        size = 28+shift\n",
        "        X_train = shifted(X_train, shift)\n",
        "        X_val = shifted(X_val, shift)\n",
        "    return (X_train, y1), (X_val, y2)\n",
        "\n",
        "# Example Usage:\n",
        "# train, validation = get_MNIST_data()\n",
        "\n",
        "def run_keras_fc_mnist(train, test, layers, epochs, split=0.1, verbose=True, trials=1):\n",
        "    (X_train, y1), (X_val, y2) = train, test\n",
        "    # Flatten the images\n",
        "    m = X_train.shape[1]\n",
        "    X_train = X_train.reshape((X_train.shape[0], m*m))\n",
        "    X_val = X_val.reshape((X_val.shape[0], m*m))\n",
        "    # Categorize the labels\n",
        "    num_classes = 10\n",
        "    y_train = np_utils.to_categorical(y1, num_classes)\n",
        "    y_val = np_utils.to_categorical(y2, num_classes)\n",
        "    # Train, use split for validation\n",
        "    val_acc, test_acc = 0, 0\n",
        "    for trial in range(trials):\n",
        "        # Reset the weights\n",
        "        # See https://github.com/keras-team/keras/issues/341\n",
        "        session = K.get_session()\n",
        "        for layer in layers:\n",
        "            for v in layer.__dict__:\n",
        "                v_arg = getattr(layer, v)\n",
        "                if hasattr(v_arg, 'initializer'):\n",
        "                    initializer_func = getattr(v_arg, 'initializer')\n",
        "                    initializer_func.run(session=session)\n",
        "        # Run the model\n",
        "        model, history, vacc, tacc = \\\n",
        "                run_keras(X_train, y_train, X_val, y_val, None, None, layers, epochs, split=split, verbose=verbose)\n",
        "        val_acc += vacc if vacc else 0\n",
        "        test_acc += tacc if tacc else 0\n",
        "    if val_acc:\n",
        "        print (\"\\nAvg. validation accuracy:\"  + str(val_acc/trials))\n",
        "    if test_acc:\n",
        "        print (\"\\nAvg. test accuracy:\"  + str(test_acc/trials))\n",
        "\n",
        "def run_keras_cnn_mnist(train, test, layers, epochs, split=0.1, verbose=True, trials=1):\n",
        "    # Load the dataset\n",
        "    (X_train, y1), (X_val, y2) = train, test\n",
        "    # Add a final dimension indicating the number of channels (only 1 here)\n",
        "    m = X_train.shape[1]\n",
        "    X_train = X_train.reshape((X_train.shape[0], m, m, 1))\n",
        "    X_val = X_val.reshape((X_val.shape[0], m, m, 1))\n",
        "    # Categorize the labels\n",
        "    num_classes = 10\n",
        "    y_train = np_utils.to_categorical(y1, num_classes)\n",
        "    y_val = np_utils.to_categorical(y2, num_classes)\n",
        "    # Train, use split for validation\n",
        "    val_acc, test_acc = 0, 0\n",
        "    for trial in range(trials):\n",
        "        # Reset the weights\n",
        "        # See https://github.com/keras-team/keras/issues/341\n",
        "        session = K.get_session()\n",
        "        for layer in layers:\n",
        "            for v in layer.__dict__:\n",
        "                v_arg = getattr(layer, v)\n",
        "                if hasattr(v_arg, 'initializer'):\n",
        "                    initializer_func = getattr(v_arg, 'initializer')\n",
        "                    initializer_func.run(session=session)\n",
        "        # Run the model\n",
        "        model, history, vacc, tacc = \\\n",
        "                run_keras(X_train, y_train, X_val, y_val, None, None, layers, epochs, split=split, verbose=verbose)\n",
        "        val_acc += vacc if vacc else 0\n",
        "        test_acc += tacc if tacc else 0\n",
        "    if val_acc:\n",
        "        print (\"\\nAvg. validation accuracy:\"  + str(val_acc/trials))\n",
        "    if test_acc:\n",
        "        print (\"\\nAvg. test accuracy:\"  + str(test_acc/trials))\n",
        "\n",
        "# Example usage:\n",
        "# train, validation = get_MNIST_data()\n",
        "# layers = [Dense(input_dim=???, units=???, activation='softmax')]\n",
        "# run_keras_fc_mnist(train, validation, layers, 1, split=0.1, verbose=True, trials=5)\n",
        "# Same pattern applies to the function: run_keras_cnn_mnist\n",
        "\n",
        "######################################################################\n",
        "# Plotting Functions\n",
        "######################################################################\n",
        "\n",
        "def plot_heat(X, y, model, res = 200):\n",
        "    eps = .1\n",
        "    xmin = np.min(X[:,0]) - eps; xmax = np.max(X[:,0]) + eps\n",
        "    ymin = np.min(X[:,1]) - eps; ymax = np.max(X[:,1]) + eps\n",
        "    ax = tidyPlot(xmin, xmax, ymin, ymax, xlabel = 'x', ylabel = 'y')\n",
        "    xl = np.linspace(xmin, xmax, res)\n",
        "    yl = np.linspace(ymin, ymax, res)\n",
        "    xx, yy = np.meshgrid(xl, yl, sparse=False)\n",
        "    zz = np.argmax(model.predict(np.c_[xx.ravel(), yy.ravel()]), axis=1)\n",
        "    im = ax.imshow(np.flipud(zz.reshape((res,res))), interpolation = 'none',\n",
        "                   extent = [xmin, xmax, ymin, ymax],\n",
        "                   cmap = 'viridis')\n",
        "    plt.colorbar(im)\n",
        "    for yi in set([int(_y) for _y in set(y)]):\n",
        "        color = ['r', 'g', 'b'][yi]\n",
        "        marker = ['X', 'o', 'v'][yi]\n",
        "        cl = np.where(y==yi)\n",
        "        ax.scatter(X[cl,0], X[cl,1], c = color, marker = marker, s=80,\n",
        "                   edgecolors = 'none')\n",
        "    return ax\n",
        "\n",
        "def tidyPlot(xmin, xmax, ymin, ymax, center = False, title = None,\n",
        "                 xlabel = None, ylabel = None):\n",
        "    plt.figure(facecolor=\"white\")\n",
        "    ax = plt.subplot()\n",
        "    if center:\n",
        "        ax.spines['left'].set_position('zero')\n",
        "        ax.spines['right'].set_color('none')\n",
        "        ax.spines['bottom'].set_position('zero')\n",
        "        ax.spines['top'].set_color('none')\n",
        "        ax.spines['left'].set_smart_bounds(True)\n",
        "        ax.spines['bottom'].set_smart_bounds(True)\n",
        "        ax.xaxis.set_ticks_position('bottom')\n",
        "        ax.yaxis.set_ticks_position('left')\n",
        "    else:\n",
        "        ax.spines[\"top\"].set_visible(False)    \n",
        "        ax.spines[\"right\"].set_visible(False)    \n",
        "        ax.get_xaxis().tick_bottom()  \n",
        "        ax.get_yaxis().tick_left()\n",
        "    eps = .05\n",
        "    plt.xlim(xmin-eps, xmax+eps)\n",
        "    plt.ylim(ymin-eps, ymax+eps)\n",
        "    if title: ax.set_title(title)\n",
        "    if xlabel: ax.set_xlabel(xlabel)\n",
        "    if ylabel: ax.set_ylabel(ylabel)\n",
        "    return ax\n",
        "\n",
        "def plot_separator(ax, th, th_0):\n",
        "    xmin, xmax = ax.get_xlim()\n",
        "    ymin,ymax = ax.get_ylim()\n",
        "    pts = []\n",
        "    eps = 1.0e-6\n",
        "    # xmin boundary crossing is when xmin th[0] + y th[1] + th_0 = 0\n",
        "    # that is, y = (-th_0 - xmin th[0]) / th[1]\n",
        "    if abs(th[1,0]) > eps:\n",
        "        pts += [np.array([x, (-th_0 - x * th[0,0]) / th[1,0]]) \\\n",
        "                                                        for x in (xmin, xmax)]\n",
        "    if abs(th[0,0]) > 1.0e-6:\n",
        "        pts += [np.array([(-th_0 - y * th[1,0]) / th[0,0], y]) \\\n",
        "                                                         for y in (ymin, ymax)]\n",
        "    in_pts = []\n",
        "    for p in pts:\n",
        "        if (xmin-eps) <= p[0] <= (xmax+eps) and \\\n",
        "           (ymin-eps) <= p[1] <= (ymax+eps):\n",
        "            duplicate = False\n",
        "            for p1 in in_pts:\n",
        "                if np.max(np.abs(p - p1)) < 1.0e-6:\n",
        "                    duplicate = True\n",
        "            if not duplicate:\n",
        "                in_pts.append(p)\n",
        "    if in_pts and len(in_pts) >= 2:\n",
        "        # Plot separator\n",
        "        vpts = np.vstack(in_pts)\n",
        "        ax.plot(vpts[:,0], vpts[:,1], 'k-', lw=2)\n",
        "        # Plot normal\n",
        "        vmid = 0.5*(in_pts[0] + in_pts[1])\n",
        "        scale = np.sum(th*th)**0.5\n",
        "        diff = in_pts[0] - in_pts[1]\n",
        "        dist = max(xmax-xmin, ymax-ymin)\n",
        "        vnrm = vmid + (dist/10)*(th.T[0]/scale)\n",
        "        vpts = np.vstack([vmid, vnrm])\n",
        "        ax.plot(vpts[:,0], vpts[:,1], 'k-', lw=2)\n",
        "        # Try to keep limits from moving around\n",
        "        ax.set_xlim((xmin, xmax))\n",
        "        ax.set_ylim((ymin, ymax))\n",
        "    else:\n",
        "        print('Separator not in plot range')\n",
        "\n",
        "def plot_decision(data, cl, diff=False):\n",
        "    layers = archs(cl)[0]\n",
        "    X, y, model = run_keras_2d(data, layers, 10, trials=1, verbose=False, display=False)\n",
        "    ax = plot_heat(X,y,model)\n",
        "    W = layers[0].get_weights()[0]\n",
        "    W0 = layers[0].get_weights()[1].reshape((cl,1))\n",
        "    if diff:\n",
        "        for i,j in list(itertools.combinations(range(cl),2)):\n",
        "            plot_separator(ax, W[:,i:i+1] - W[:,j:j+1], W0[i:i+1,:] - W0[j:j+1,:])\n",
        "    else:\n",
        "        for i in range(cl):\n",
        "            plot_separator(ax, W[:,i:i+1], W0[i:i+1,:])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "y-Tigvq4gOEs"
      },
      "outputs": [],
      "source": [
        "import math as m\n",
        "\n",
        "class Sequential:\n",
        "    def __init__(self, modules, loss):\n",
        "        self.modules = modules\n",
        "        self.loss = loss\n",
        "\n",
        "    def mini_gd(self, X, Y, iters, lrate, notif_each=None, K=10):\n",
        "        D, N = X.shape\n",
        "\n",
        "        np.random.seed(0)\n",
        "        num_updates = 0\n",
        "        indices = np.arange(N)\n",
        "        while num_updates < iters:\n",
        "\n",
        "            np.random.shuffle(indices)\n",
        "            X = X[:,indices]  # Your code\n",
        "            Y = Y[:,indices]  # Your code\n",
        "\n",
        "            for j in range(m.floor(N/K)):\n",
        "                if num_updates >= iters: break\n",
        "\n",
        "                # Implement the main part of mini_gd here\n",
        "                # Your code\n",
        "                Xt = X[:, j*K:(j+1)*K]\n",
        "                Yt = Y[:, j*K:(j+1)*K]\n",
        "\n",
        "                Ypred = self.forward(Xt)\n",
        "                loss = self.loss.forward(Ypred, Yt)\n",
        "\n",
        "                err = self.loss.backward()\n",
        "                self.backward(err)\n",
        "                self.step(lrate)\n",
        "                num_updates += 1\n",
        "\n",
        "    def forward(self, Xt):\n",
        "        for m in self.modules: Xt = m.forward(Xt)\n",
        "        return Xt\n",
        "\n",
        "    def backward(self, delta):\n",
        "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
        "\n",
        "    def step(self, lrate):\n",
        "        for m in self.modules: m.step(lrate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JZeeKXkm6YI"
      },
      "source": [
        "<b>2B)</b> We are now ready to implement batch normalization into our neural network framework! Our module `BatchNorm` will sit between consecutive layers of neurons, such as the $l^{th}$ and $(l+1)^{th}$ layers, acting as a \"corrector\" which allows $W^l$ to change freely, producing outputs $z^l$, but then the module corrects the covariate shift induced in the signals before they reach the $(l+1)^{th}$ layer, converting $z^l$ to $\\widehat{Z}^l$.\n",
        "\n",
        "The following is a summmary what is described in the <a href=\"https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week7/neural_networks_2/2\">lecture notes</a>, and it should guide your implementation of the module.\n",
        "\n",
        "Any normalization between the $l^{th}$ and $(l+1)^{th}$ layers is done *separately* for each of the $n^l$ input connections leading to the $(l+1)^{th}$ layer. We handle a mini-batch of data of size $K$, and $Z^l$ is $n^l \\times K$, and the output $\\widehat{Z}^l$is of the same shape.\n",
        "\n",
        "We first compute $n^l$ *batchwise* means and\n",
        "standard deviations.  Let $\\mu^l$ be the $n^l \\times 1$ vector (`self.mus`) where\n",
        "$$\\mu^l_i = \\frac{1}{K} \\sum_{j = 1}^K Z^l_{ij}\\;\\;,$$\n",
        "and let $\\sigma^l$ be the $n^l \\times 1$ vector (`self.vars`) where\n",
        "$$\\sigma^l_i = \\sqrt{\\frac{1}{K} \\sum_{j = 1}^K (Z^l_{ij} - \\mu_i)^2}\\;\\;.$$\n",
        "\n",
        "The normalized data `self.norm` is the matrix $\\overline{Z}$, where\n",
        "$$\\overline{Z}^l_{ij} = \\frac{Z^l_{ij} - \\mu^l_i}{\\sigma^l_i + \\epsilon}\\;\\;,$$\n",
        "and where $\\epsilon$ is a very small constant to guard against division by\n",
        "zero.\n",
        "\n",
        "We define weights $G^l$ (`self.G`) and $B^l$ (`self.B`), each being an $n^l \\times 1$ vector, which we use to to shift and scale the outputs:\n",
        "$$\\widehat{Z}^l_{ij} = G^l_i \\overline{Z}^l_{ij} + B^l_i\\;\\;.$$\n",
        "\n",
        "The outputs are finally ready to be passed to the $(l+1)^{th}$ layer.\n",
        "\n",
        "A slight warning (that we will not worry about here) about `BatchNorm` is that during the *test* phase, if the test mini-batch size is too small (imagine we are deploying a neural network that deals with live video frames), then the lack of samples would cause the freshly-calculated $\\mu^l$ and $\\sigma^l$ to be far off from their true values that the module's parameters $G^l$ and $B^l$ were trained to be compatible with. To fix that, people usually compute a running average of $\\mu^l$ and $\\sigma^l$ during training, to be used at test time. We will assume our test mini-batches are large enough.\n",
        "\n",
        "In this problem we only implement the `BatchNorm.forward` and `BatchNorm.step` methods. We provide you with the implementation for `BatchNorm.backward` and the lecture notes contain the details of the derivations. You will need to fill in the missing code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "UlXP26plm8R7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Module' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBatchNorm\u001b[39;00m(Module):    \n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, m):\n\u001b[1;32m      3\u001b[0m         np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Module' is not defined"
          ]
        }
      ],
      "source": [
        "class BatchNorm(Module):    \n",
        "    def __init__(self, m):\n",
        "        np.random.seed(0)\n",
        "        self.eps = 1e-20\n",
        "        self.m = m  # number of input channels\n",
        "        \n",
        "        # Init learned shifts and scaling factors\n",
        "        self.B = np.zeros([self.m, 1])\n",
        "        self.G = np.random.normal(0, 1.0 * self.m ** (-.5), [self.m, 1])\n",
        "        \n",
        "    # Works on m x b matrices of m input channels and b different inputs\n",
        "    def forward(self, A):# A is m x K: m input channels and mini-batch size K\n",
        "        # Store last inputs and K for next backward() call\n",
        "        self.A = A\n",
        "        self.K = A.shape[1]\n",
        "        \n",
        "        self.mus = np.mean(self.A, axis=1, keepdims=True)\n",
        "        self.vars = np.var(self.A, axis=1, keepdims=True)\n",
        "\n",
        "        # Normalize inputs using their mean and standard deviation\n",
        "        self.norm = (self.A - self.mus) / np.sqrt(self.vars+1e-10)\n",
        "            \n",
        "        # Return scaled and shifted versions of self.norm\n",
        "        return (self.G * self.norm) + self.B\n",
        "\n",
        "    def backward(self, dLdZ):\n",
        "        # Re-usable constants\n",
        "        std_inv = 1/np.sqrt(self.vars+self.eps)\n",
        "        A_min_mu = self.A-self.mus\n",
        "        \n",
        "        dLdnorm = dLdZ * self.G\n",
        "        dLdVar = np.sum(dLdnorm * A_min_mu * -0.5 * std_inv**3, axis=1, keepdims=True)\n",
        "        dLdMu = np.sum(dLdnorm*(-std_inv), axis=1, keepdims=True) + dLdVar * (-2/self.K) * np.sum(A_min_mu, axis=1, keepdims=True)\n",
        "        dLdX = (dLdnorm * std_inv) + (dLdVar * (2/self.K) * A_min_mu) + (dLdMu/self.K)\n",
        "        \n",
        "        self.dLdB = np.sum(dLdZ, axis=1, keepdims=True)\n",
        "        self.dLdG = np.sum(dLdZ * self.norm, axis=1, keepdims=True)\n",
        "        return dLdX\n",
        "\n",
        "    def step(self, lrate):\n",
        "        self.B -= lrate*self.dLdB\n",
        "        self.G -= lrate*self.dLdG\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65LKUAHD_77Y"
      },
      "source": [
        "# 3) 2D Datasets\n",
        "\n",
        "For the 2D datasets, we have provided the following function:\n",
        "\n",
        "\n",
        "```\n",
        "run_keras_2d(data_name, layers, epochs, split=0.25, display=True, trials=5)\n",
        "```\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "data_name is a string, such as '1', '2', etc.\n",
        "layers is a list of Keras layer definitions for a Sequential model, e.g.\n",
        "```\n",
        "[Dense(input_dim=2, units=10, activation='relu'), Dense(units=2, activation='softmax')]\n",
        "```\n",
        "\n",
        "epochs is an integer indicating how many times to go through the data in training\n",
        "split is a fraction of the training data to use for validation if a validation set is not defined\n",
        "display whether to display result plots\n",
        "verbose whether to print loss and accuracy (percent correctly labeled) each epoch\n",
        "trials is an integer indicating how many times to perform the training and testing\n",
        "2D Data\n",
        "The two-class datasets have data_names: '1','2','3','4'. Target accuracies (percent correct) on the validation set are (99%, 90.5%, 96%, 94%).\n",
        "\n",
        "In this problem, try the following 5 architectures, specified by the number of units in the hidden layers:\n",
        "\n",
        "1: (0), 2: (10), 3: (100), 4: (10, 10), 5: (100, 100))\n",
        "You may find the archs function in the code file to be helpful here.\n",
        "Some of these questions ask for the \"simplest\" architecture; the list above is ordered starting with the simplest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Vp-RWPD7FUeo",
        "outputId": "b944266c-d0eb-4f17-9a63-71b26d3110d7"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mTraceback (most recent call last):\n",
            "\u001b[1;31m  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "\u001b[1;31m  File \"<frozen runpy>\", line 88, in _run_code\n",
            "\u001b[1;31m  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
            "\u001b[1;31m    app.launch_new_instance()\n",
            "\u001b[1;31m  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1074, in launch_instance\n",
            "\u001b[1;31m    app.initialize(argv)\n",
            "\u001b[1;31m  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 118, in inner\n",
            "\u001b[1;31m    return method(app, *args, **kwargs)\n",
            "\u001b[1;31m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[1;31m  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 669, in initialize\n",
            "\u001b[1;31m    self.init_kernel()\n",
            "\u001b[1;31m  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 555, in init_kernel\n",
            "\u001b[1;31m    kernel = kernel_factory(\n",
            "\u001b[1;31m             ^^^^^^^^^^^^^^^\n",
            "\u001b[1;31m  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/configurable.py\", line 583, in instance\n",
            "\u001b[1;31m    inst = cls(*args, **kwargs)\n",
            "\u001b[1;31m           ^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[1;31m  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 148, in __init__\n",
            "\u001b[1;31m    if _use_appnope() and self._darwin_app_nap:\n",
            "\u001b[1;31m       ^^^^^^^^^^^^^^\n",
            "\u001b[1;31m  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/eventloops.py\", line 21, in _use_appnope\n",
            "\u001b[1;31m    return sys.platform == \"darwin\" and V(platform.mac_ver()[0]) >= V(\"10.9\")\n",
            "\u001b[1;31m                                        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[1;31m  File \"/opt/anaconda3/lib/python3.12/site-packages/packaging/version.py\", line 200, in __init__\n",
            "\u001b[1;31m    raise InvalidVersion(f\"Invalid version: '{version}'\")\n",
            "\u001b[1;31mpackaging.version.InvalidVersion: Invalid version: ''. \n",
            "\u001b[1;31mDaha fazla ayrıntı için Jupyter <a href='command:jupyter.viewOutput'>günlüğüne</a> bakın."
          ]
        }
      ],
      "source": [
        "from code_for_hw8_keras import *\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.layers import Conv1D, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import Callback\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "from keras.initializers import VarianceScaling\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.layers import Conv1D, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.initializers import VarianceScaling\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "_YWbWX47_9Tr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keras FC: dataset= 3\n",
            "Convert from -1,1 to 0,1\n",
            "Loading X (400, 2) y (400,) classes {0.0, 1.0}\n",
            "Convert from -1,1 to 0,1\n",
            "Loading X (200, 2) y (200,) classes {0.0, 1.0}\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'np_utils' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#example of run_keras_2d\u001b[39;00m\n\u001b[1;32m      2\u001b[0m layer1 \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m run_keras_2d(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m\"\u001b[39m, archs(\u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m10\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, display\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/GitHub/My_code2/4_MIT_AI_lectures/8_week/code_for_hw8/code_for_hw8_keras.py:132\u001b[0m, in \u001b[0;36mrun_keras_2d\u001b[0;34m(data_name, layers, epochs, display, split, verbose, trials)\u001b[0m\n\u001b[1;32m    130\u001b[0m X_test, y3, _ \u001b[38;5;241m=\u001b[39m get_data_set(test_dataset)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Categorize the labels\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np_utils\u001b[38;5;241m.\u001b[39mto_categorical(y, num_classes) \u001b[38;5;66;03m# one-hot\u001b[39;00m\n\u001b[1;32m    133\u001b[0m y_val \u001b[38;5;241m=\u001b[39m y_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np_utils' is not defined"
          ]
        }
      ],
      "source": [
        "#example of run_keras_2d\n",
        "layer1 = keras.layers.Dense(units=3, activation='relu', use_bias=False)\n",
        "run_keras_2d(\"3\", archs(2)[0], 10, split=0.5, display=False, verbose=False, trials=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh4u39OCjLza"
      },
      "source": [
        "# Weight sharing (OPTIONAL)\n",
        "\n",
        "** Note: You can click the arrow on the left of this text block to collapse/expand this optional section and all its code blocks **\n",
        "\n",
        "In the lab we designed a CNN that can count the number of objects in 1 dimensional images, where each black pixel is represented by a value of 0 and each white pixel is represented by a value of 1. Recall that an object is a consecutive sequence of black pixels ($0$'s). For example, the sequence $0100110$ contains three objects.\n",
        "\n",
        "Here we want to see how hard/easy it is to train such a network from data.  Our network architecture will be as follows:\n",
        "\n",
        "* The first layer is convolutional and you will implement it using the Keras `Conv1D` function, with a kernel of size 2 and stride of 1 with ReLu activation.\n",
        "\n",
        "* The second layer is a fully connected `Dense` layer which has a scalar output.\n",
        "\n",
        "Here is sample usage of the `Conv1D` and`Dense` layers.\n",
        "\n",
        "`layer1=keras.layers.Conv1D(filters=?, kernel_size=?, strides=?,use_bias=False, activation=?, batch_size=1, input_shape=?, padding='same')`\n",
        "\n",
        "`Dense(units=?, activation=?, use_bias=False)`\n",
        "\n",
        "You need to fill in the parameters marked with `?` based on the problem specifications. Note also that in Keras, depending on your implementation, you may be forced to use *three* layers to implement such a network, where one intermediary `Flatten` layer is used to flatten the output of the convolutional layer, before being passed to the dense layer.\n",
        "\n",
        "Refer to the <a href=\"https://keras.io/layers/convolutional/\">Conv 1D</a>, <a href=\"https://keras.io/layers/core/\">Dense</a> and <a href=\"https://keras.io/layers/core/#flatten\">Flatten</a> descriptions in the Keras documentation to see the available parameter options.\n",
        "\n",
        "In this exercise, we fix the structure and want to learn the best combination of weights from data. In the homework code, we have provided functions `train_neural_counter` and `get_image_data_1d`. You can use them to generate data and train the above neural network in Keras to answer the following questions. We assume that the images in our data set are randomly generated. The probability of a pixel being white is $0.1$. We work with mean squared error as the loss function for this problem. We have provided template code which you can fill in, to perform the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKa8iMv_j3ek"
      },
      "source": [
        "<b>4B)</b> What is (approximately) the expected loss of the network on $1024\\times 1$ images if the convolutional layer is an averaging filter and second layer is the sum function (without a bias term)? (Note that you can answer the question theoretically or through coding, depending on your preference.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKxedp-qFXJe"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKPcB588ok8a"
      },
      "outputs": [],
      "source": [
        "# Code template if you would like to check 4B) through code\n",
        "\n",
        "imsize = 1024\n",
        "prob_white = 0.1\n",
        "\n",
        "num_filters = None  # Your code\n",
        "kernel_size = None  # Your code\n",
        "strides = None  # Your code\n",
        "activation_conv = None  # Your code\n",
        "\n",
        "(X_train,Y_train,X_val,Y_val,X_test,Y_test) = get_image_data_1d(1000,imsize,prob_white)\n",
        "\n",
        "layer1=keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, \\\n",
        "       strides=strides, use_bias=False, activation=activation_conv, batch_size=1, input_shape=(imsize,1), padding='same')\n",
        "\n",
        "activation_dense = None  # Your code\n",
        "num_units = None  # Your code\n",
        "layer3=Dense(units=num_units, activation=activation_dense, use_bias=False)\n",
        "\n",
        "layers=[layer1,Flatten(),layer3]\n",
        "\n",
        "# This is how we create the model using our layers\n",
        "model=Sequential()\n",
        "for layer in layers:\n",
        "    model.add(layer)\n",
        "\n",
        "model.compile(loss='mse', optimizer=Adam())\n",
        "\n",
        "# Set the weights of the layers to desired values\n",
        "# We give you the lines to use for this part\n",
        "model.layers[0].set_weights([np.array([1/2,1/2]).reshape(2,1,1)])\n",
        "model.layers[-1].set_weights([np.ones(imsize).reshape(imsize,1)])\n",
        "\n",
        "model.evaluate(X_test,Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js3OYsbwj7Ms"
      },
      "source": [
        "<b>4C)</b> Now suppose we add a bias term of $-10$ to the last layer. What is (approximately) the expected quadratic loss? (Note that you can answer the question theoretically or through coding, depending on your preference.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKynxhF1klga"
      },
      "outputs": [],
      "source": [
        "# Edit code from 4B) with the bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GCLr8qmj-Hk"
      },
      "source": [
        "<b>4D)</b> Averaging type filters are abundant and form a nearly flat valley of local minima for this problem. It is difficult for the network to find alternative solutions on its own. We need to force our way out of these bad minima and towards a better solution, i.e., an edge detector. To force the first layer to behave as an edge detector, we need to choose a proper **kernel regularizer**. Consider the following functions\n",
        "\n",
        "$f_1=\\sum_i |w_i|$, $f_2=\\sum_i |w_i^2|$, $f_3=|\\sum_{i} w_i|$. Which one of the choices is likely to guide the network to find an edge detector at the convolution layer?\n",
        "\n",
        "\n",
        "<a href=\"https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week8/week8_homework/\">Refer to Catsoop.</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aubU6Q6kwOI"
      },
      "source": [
        "Implement your choice of regularizers from above in the code (complete the function `filter_reg`). Do not allow any bias in the layers for the rest of the problem. The code generates some random test and training data sets and trains the model on these data. Run a few learning trials (5 or more) for each data set and answer the following questions based on the performance of your model.\n",
        "\n",
        "**IMPORTANT**: When implementing `filter_reg`, you should use the keras backend operations, imported as \"K\" in the code. So for example, `K.sum` and `K.abs`, rather than `np.sum` and `np.abs`. This is because the `weights` argument is NOT a numpy object, but rather an internal Keras object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOLZf_JsuTLn"
      },
      "outputs": [],
      "source": [
        "# Implement filter_reg\n",
        "\n",
        "def filter_reg(weights):\n",
        "    # We scale the output of the filter by lam\n",
        "    lam=1000\n",
        "    filter_result = None  # Your code\n",
        "    return lam * filter_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYRwd0eJkAdh"
      },
      "source": [
        "<b>4E)</b> For $1024\\times 1$ images and training set of size $1000$, is the network **without any regularization** likely to find models that have a mean square error lower than 8 on the test data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KiCbZmksXO6"
      },
      "outputs": [],
      "source": [
        "# Code template if you would like to check 4B) through code\n",
        "\n",
        "imsize = 1024\n",
        "prob_white = 0.1\n",
        "\n",
        "data=get_image_data_1d(1000, imsize, prob_white)\n",
        "trials=5\n",
        "for trial in range(trials):\n",
        "\n",
        "    num_filters = None  # Your code\n",
        "    kernel_size = None  # Your code\n",
        "    strides = None  # Your code\n",
        "    activation_conv = None  # Your code\n",
        "\n",
        "    layer1=keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, \\\n",
        "    strides=strides, use_bias=False, activation=activation_conv, batch_size=1, \\\n",
        "    input_shape=(imsize,1),padding='same')\n",
        "\n",
        "    activation_dense = None  # Your code\n",
        "    num_units = None  # Your code\n",
        "\n",
        "    layer3=Dense(units=num_units, activation=activation_dense, use_bias=False)\n",
        "\n",
        "    layers=[layer1,Flatten(),layer3]\n",
        "    model,err = train_neural_counter(layers, data, 'mse')\n",
        "\n",
        "    model.layers[0].get_weights()[0]\n",
        "    np.mean(model.layers[-1].get_weights()[0])\n",
        "    print(err)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1vcUEL-vW9D"
      },
      "source": [
        "#### For parts F) to J), simply edit your code from E) with the necessary changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_25ygQJkD5F"
      },
      "source": [
        "<b>4F)</b> Repeat the same experiment, but now with the regularizer you implemented. Try different regularization parameters. Which choice of regularization parameter gives the best prediction results?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNAChIqylIlt"
      },
      "outputs": [],
      "source": [
        "# Edit code from 4E), using your filter as the kernel_regularizer in the Conv1D layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs44ze96kHZZ"
      },
      "source": [
        "<b>4G)</b> With the above choice of regularization parameter, what is the mean square error of the best network that you find on the test data? Try a few trials (5 or more) for each data test and report the value of the best network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAN0k9wylOmz"
      },
      "source": [
        "\n",
        "#### We expect the training to be easier when there are fewer parameters to learn. Consider images of size $128\\times 1$ for the rest of the problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnktFwXRkKNF"
      },
      "source": [
        "<b>4H)</b> Instead of resorting to regularization again, we may instead find a way to reduce the number of parameters. What additional layer can you add to the output of the convolution layer to reduce the number of parameters to be learned without losing any relevant information?\n",
        "\n",
        "<a href=\"https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week8/week8_homework/\">Refer to Catsoop.</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXgOqKtRkNRP"
      },
      "source": [
        "<b>4I)</b> Add the layer you suggested above to your network and run some tests with data sets of size 1000 on $128\\times 1$ images.  How many parameters are left to learn with the new structure?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8FRQawHkPG9"
      },
      "source": [
        "<b>4J)</b> Mark your observations on the two structures (not using regularization).\n",
        "<a href=\"https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week8/week8_homework/\">Refer to Catsoop.</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-iTDCrHySde"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQGlJLxI__4A"
      },
      "source": [
        "# 5) MNIST (Digit Classification)\n",
        "\n",
        "In this section, we'll be looking at the MNIST data set seen already in problem 2. This time, we look at the *complete* MNIST problem where our networks will take an image of *any* digit from $0-9$ as input (recall that problem 2 only looked at digits $0$ and $1$) and try to predict that digit. Also, we will now use out-of-the-box neural network implementations using Keras and Tensorflow. State-of-the-art systems have error rates of less that one half of one percent on this data set (see <a href=\"http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354\">this list</a>).  We'll be happy with an error rate less than 2% since we don't have all year...\n",
        "<br>\n",
        "\n",
        "You can access the MNIST data for this problem using:\n",
        "<br><code>train, validation = get_MNIST_data()</code>\n",
        "<br>\n",
        "\n",
        "You can run the fully connected MNIST model, using:\n",
        "<br><code>run_keras_fc_mnist(train, validation, layers, epochs, split=0.1, trials=5)</code>\n",
        "<br>\n",
        "\n",
        "And, you can run the CNN MNIST test, using:\n",
        "<br><code>run_keras_cnn_mnist(train, validation, layers, epochs, split=0.1, trials=5)</code>\n",
        "<br>\n",
        "\n",
        "For all following experiments, please run for 5 trials (use `trials=5`) and report the average accuracy.\n",
        "<br>\n",
        "\n",
        "A word of warning, if you have a machine with a single core and/or very little RAM, you'll be better off running on an Athena workstation. If your solutions are not being accepted, and you are confident in your approach, try with more trials. Also,\n",
        "<br>\n",
        "\n",
        "You will need to design your own `layers` to feed to `run_keras_fc_mnist` and `run_keras_cnn_mnist`, which will be different than the ones specified by `archs()`. For instance, `layers=[Dense(input_dim=64, units=4, activation=\"softmax\")]` defines a single layer with 64 inputs, 4 output units, and softmax activation. Also, we advise you to use the option `verbose=True` when unsure about the progress made during training of your models.\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx1jt6P9AUk1"
      },
      "source": [
        "<b> 5A)</b> Look at the code and indicate what the difference is between <code>run_keras_fc_mnist</code> and <code>run_keras_cnn_mnist</code>? <a href=\"https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week8/week8_homework/\">Refer to the HW8 page.</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5moSfb7CcXd"
      },
      "outputs": [],
      "source": [
        "def run_keras_fc_mnist(train, test, layers, epochs, split=0.1, verbose=True, trials=1):\n",
        "    '''\n",
        "    train, test = input data\n",
        "    layers = list of Keras layers, e.g. [Dense(32, input_dim=784), Dense(10)]\n",
        "    epochs = number of epochs to run the model for each traini\n",
        "    ng trial\n",
        "    trials = number of evaluation trials, resetting weights before each trial\n",
        "    '''\n",
        "    (X_train, y1), (X_val, y2) = train, test\n",
        "    # Flatten the images\n",
        "    m = X_train.shape[1]\n",
        "    X_train = X_train.reshape((X_train.shape[0], m*m))\n",
        "    X_val = X_val.reshape((X_val.shape[0], m*m))\n",
        "    # Categorize the labels\n",
        "    num_classes = 10\n",
        "    y_train = np_utils.to_categorical(y1, num_classes)\n",
        "    y_val = np_utils.to_categorical(y2, num_classes)\n",
        "    # Train, use split for validation\n",
        "    val_acc, test_acc = 0, 0\n",
        "    for trial in range(trials):\n",
        "        # Reset the weights\n",
        "        # See https://github.com/keras-team/keras/issues/341\n",
        "        session = K.get_session()\n",
        "        for layer in layers:\n",
        "            for v in layer.__dict__:\n",
        "                v_arg = getattr(layer, v)\n",
        "                if hasattr(v_arg, 'initializer'):\n",
        "                    initializer_func = getattr(v_arg, 'initializer')\n",
        "                    initializer_func.run(session=session)\n",
        "        # Run the model\n",
        "        model, history, vacc, tacc = \\\n",
        "                run_keras(X_train, y_train, X_val, y_val, None, None, layers, epochs, split=split, verbose=verbose)\n",
        "        val_acc += vacc if vacc else 0\n",
        "        test_acc += tacc if tacc else 0\n",
        "    if val_acc:\n",
        "        print (\"\\nAvg. validation accuracy:\"  + str(val_acc/trials))\n",
        "    if test_acc:\n",
        "        print (\"\\nAvg. test accuracy:\"  + str(test_acc/trials))\n",
        "\n",
        "def run_keras_cnn_mnist(train, test, layers, epochs, split=0.1, verbose=True, trials=1):\n",
        "    # Load the dataset\n",
        "    (X_train, y1), (X_val, y2) = train, test\n",
        "    # Add a final dimension indicating the number of channels (only 1 here)\n",
        "    m = X_train.shape[1]\n",
        "    X_train = X_train.reshape((X_train.shape[0], m, m, 1))\n",
        "    X_val = X_val.reshape((X_val.shape[0], m, m, 1))\n",
        "    # Categorize the labels\n",
        "    num_classes = 10\n",
        "    y_train = np_utils.to_categorical(y1, num_classes)\n",
        "    y_val = np_utils.to_categorical(y2, num_classes)\n",
        "    # Train, use split for validation\n",
        "    val_acc, test_acc = 0, 0\n",
        "    for trial in range(trials):\n",
        "        # Reset the weights\n",
        "        # See https://github.com/keras-team/keras/issues/341\n",
        "        session = K.get_session()\n",
        "        for layer in layers:\n",
        "            for v in layer.__dict__:\n",
        "                v_arg = getattr(layer, v)\n",
        "                if hasattr(v_arg, 'initializer'):\n",
        "                    initializer_func = getattr(v_arg, 'initializer')\n",
        "                    initializer_func.run(session=session)\n",
        "        # Run the model\n",
        "        model, history, vacc, tacc = \\\n",
        "                run_keras(X_train, y_train, X_val, y_val, None, None, layers, epochs, split=split, verbose=verbose)\n",
        "        val_acc += vacc if vacc else 0\n",
        "        test_acc += tacc if tacc else 0\n",
        "    if val_acc:\n",
        "        print (\"\\nAvg. validation accuracy:\"  + str(val_acc/trials))\n",
        "    if test_acc:\n",
        "        print (\"\\nAvg. test accuracy:\"  + str(test_acc/trials))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sGfqAbICbmE"
      },
      "source": [
        "<b> 5B)</b> Using one epoch of training, what is the accuracy of a network **with no hidden units** (using the <code>run_keras_fc_mnist</code> method) on this data? Hint: this is expected to be terrible. If it's still not working, run for more trials. Remember to use 10 output units (the network predicts a digit from 0-9) and softmax activation!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1VAxZ17DtPQ"
      },
      "outputs": [],
      "source": [
        "layers = None  # Your code\n",
        "train, validation = get_MNIST_data()\n",
        "run_keras_fc_mnist(train, validation, layers, 1, split=0.1, verbose=True, trials=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gndf-ASDEe_"
      },
      "source": [
        "<b> 5C)</b> When creating the keras layer, pass in the following argument to Dense:\n",
        "<code>kernel_initializer=VarianceScaling(scale=0.001, mode='fan_in', distribution='normal', seed=None)</code> and repeat the test.  What is the accuracy now?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2CP5WJQELJA"
      },
      "outputs": [],
      "source": [
        "layers = None  # Your code\n",
        "train, validation = get_MNIST_data()\n",
        "run_keras_fc_mnist(train, validation, layers, 1, split=0.1, verbose=True, trials=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsQ31e1lDE6u"
      },
      "source": [
        "<b> 5D)</b> Now, linearly scale the data so that the pixel values are between 0 and 1 and repeat your test with the original layer (no VarianceScaling). What is the accuracy now?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b-RcZu1EPaj"
      },
      "outputs": [],
      "source": [
        "layers = None  # Your code\n",
        "train, validation = get_MNIST_data()\n",
        "\n",
        "# Scale the images\n",
        "train = (None, train[1])  # Your code\n",
        "validation = (None, validation[1])  # Your code\n",
        "\n",
        "run_keras_fc_mnist(train, validation, layers, 1, split=0.1, verbose=True, trials=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4aBsxAzDFBY"
      },
      "source": [
        "<b> 5E)</b> What is happening? <a href=\"https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week8/week8_homework/\">Refer to the HW8 page.</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrYGfcOLEr0f"
      },
      "source": [
        "### Important: <b>Always scale the data like in 5D) for subsequent problems.</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHoyqJdqDFH5"
      },
      "source": [
        "<b> 5F)</b> Using this same architecture, evaluate validation accuracy for number of training epochs in [5, 10, 15]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8PlbWS_EwTv"
      },
      "outputs": [],
      "source": [
        "train, validation = get_MNIST_data()\n",
        "\n",
        "# Scale the images\n",
        "train = (None, train[1])  # Your code\n",
        "validation = (None, validation[1])  # Your code\n",
        "\n",
        "for epochs in [5,10,15]:\n",
        "    layers = None  # Your code\n",
        "    run_keras_fc_mnist(train, validation, layers, epochs, split=0.1, verbose=True, trials=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zJk4u2-DFNi"
      },
      "source": [
        "<b> 5G a)</b>With the validation accuracy that you just saw on per digit basis using $15$ epochs, and assuming each digit is read independently from the others, what is the probability of reading a 5 digit zip code correctly?<br>\n",
        "\n",
        "<b> 5G b)</b>Now, assume that the accuracy is 0.9985, what is the probability of reading a zip code correctly?<br>\n",
        "\n",
        "\n",
        "This is why people care about dropping the error rates to what at first sound like ridiculous values.\n",
        "\n",
        "<a href=\"https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week8/week8_homework/\">Refer to the HW8 page.</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCPTuz-tDFTg"
      },
      "source": [
        "<b> 5H)</b> Using one epoch of training, try a single hidden layer with ReLU and gradually increase the units (128, 256, 512, 1024) units.  What are the accuracies?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmpbP_VHFoh1"
      },
      "outputs": [],
      "source": [
        "train, validation = get_MNIST_data()\n",
        "\n",
        "# Scale the images\n",
        "train = (None, train[1])  # Your code\n",
        "validation = (None, validation[1])  # Your code\n",
        "\n",
        "for num in [128,256,512,1024]:\n",
        "    layers = None  # Your code\n",
        "    run_keras_fc_mnist(train, validation, layers, 1, split=0.1, verbose=True, trials=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEM5mZi5DFYS"
      },
      "source": [
        "<b> 5I)</b> Now, try a network with two layers, again using one epoch, with 512 units in the first hidden layer and and 256 units in the second hidden layer.  What is the accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cwp1VR7F06q"
      },
      "outputs": [],
      "source": [
        "train, validation = get_MNIST_data()\n",
        "\n",
        "# Scale the images\n",
        "train = (None, train[1])  # Your code\n",
        "validation = (None, validation[1])  # Your code\n",
        "\n",
        "layers = None  # Your code\n",
        "run_keras_fc_mnist(train, validation, layers, 1, split=0.1, verbose=True, trials=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmnNUT2nDFdi"
      },
      "source": [
        "<b> 5J)</b> Build a convolutional network with the following structure:\n",
        "\n",
        "<ul>\n",
        "<li> A convolutional layer with 32 filters of size 3 × 3, with a ReLU activation\n",
        "<li> A max pooling layer with size 2 × 2\n",
        "<li> A convolutional layer with 64 filters of size 3 × 3, with ReLU activation\n",
        "<li> A max pooling layer with size 2 × 2\n",
        "<li> A flatten layer\n",
        "<li> A fully connected layer with 128 neurons, with ReLU activation\n",
        "<li> A dropout layer with drop probability 0.5\n",
        "<li> A fully-connected layer with 10 neurons with softmax\n",
        "</ul>\n",
        "Train it on MNIST for one epoch, using <code>run_keras_cnn_mnist</code>.  What is the accuracy on the validation set?\n",
        "\n",
        "If you have time to run the training for more epochs, try it, you should see improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-sPW8xKF7EZ"
      },
      "outputs": [],
      "source": [
        "train, validation = get_MNIST_data()\n",
        "\n",
        "# Scale the images\n",
        "train = (None, train[1])  # Your code\n",
        "validation = (None, validation[1])  # Your code\n",
        "\n",
        "layers = None  # Your code\n",
        "\n",
        "run_keras_cnn_mnist(train, validation, layers, 1, split=0.1, verbose=True, trials=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjmqgGvIDFiS"
      },
      "source": [
        "<b> 5K)</b> Now, let's compare the performance of a fully connected model and a CNN on data where the characters have been shifted randomly so that they are no longer centered.  \n",
        "\n",
        "You can build such a data set by calling: <code>train_20, validation_20 = get_MNIST_data(shift=20)</code>. Remember to scale it appropriately.\n",
        "\n",
        "<b>Note that each image is now 48x48, so you will need to change your layer definitions</b>.\n",
        "Run your two-hidden-layer FC architecture from above (problem 5I) on this data and then run the CNN architecture from above (problem 5J), both for one epoch. Report your results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvfiyrN9Gf7X"
      },
      "outputs": [],
      "source": [
        "train_20, validation_20 = get_MNIST_data(shift=20) # Your code (fill in the shift)\n",
        "print(train_20[0].shape)\n",
        "print(validation_20[0].shape)\n",
        "# Scale the images\n",
        "train_20 = (None, train_20[1])  # Your code\n",
        "validation_20 = (None, validation_20[1])  # Your code\n",
        "\n",
        "\n",
        "\n",
        "layers_fc = None  # Your code\n",
        "\n",
        "run_keras_fc_mnist(train_20, validation_20, layers_fc, 1, split=0.1, verbose=True, trials=5)\n",
        "\n",
        "layers_cnn = None  # Your code\n",
        "\n",
        "run_keras_cnn_mnist(train_20, validation_20, layers_cnn, 1, split=0.1, verbose=True, trials=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iLmfPHaC2d8"
      },
      "source": [
        "<b> 5L)</b> Some possible conclusions. <a href=\"https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week8/week8_homework/\">Refer to the HW8 page.</a>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "v-sSs7N4mMiX",
        "Zh4u39OCjLza"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
