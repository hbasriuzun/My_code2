{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q58cS9antfCw"
      },
      "source": [
        "#MIT 6.036 Spring 2019: Homework 10#\n",
        "\n",
        "This colab notebook provides code and a framework for questions 2, 3, and 4 from [homework 10](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week10/week10_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, download the code distribution for this homework that contains test cases and helper functions.\n",
        "\n",
        "Run the next code block to download and import the code for this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OUEtSZRdtmI2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: wget\n",
            "unzip:  cannot find or open code_for_hw10.zip, code_for_hw10.zip.zip or code_for_hw10.zip.ZIP.\n",
            "zsh:1: no matches found: code_for_hw10/*\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'code_for_hw10'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munzip code_for_hw10.zip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmv code_for_hw10/* .\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcode_for_hw10\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcode_for_hw10\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmdp10\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmdp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'code_for_hw10'"
          ]
        }
      ],
      "source": [
        "!rm -rf code_for_hw10* __MACOSX data .DS_Store\n",
        "!wget --quiet https://introml_oll.odl.mit.edu/cat-soop/6.036/static/homework/hw10/code_for_hw10.zip\n",
        "!unzip code_for_hw10.zip\n",
        "!mv code_for_hw10/* .\n",
        "\n",
        "import code_for_hw10 as code_for_hw10\n",
        "import mdp10 as mdp\n",
        "\n",
        "import numpy as np\n",
        "import math as m\n",
        "import random\n",
        "\n",
        "import pdb\n",
        "from dist import uniform_dist, delta_dist, mixture_dist, DDist\n",
        "from util import argmax_with_val, argmax\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import importlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zhptv005XBN"
      },
      "source": [
        "# 2) Implement Q-Learning\n",
        "\n",
        "We'll work up to implementing the Q-learning algorithm by extending our code from HW9. In the next block, please copy and paste your implementations of the following functions from HW9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cu9UMTm2l8x"
      },
      "outputs": [],
      "source": [
        "def value_iteration(mdp, q, eps = 0.01, max_iters = 1000):\n",
        "    # Your code here (COPY FROM HW9)\n",
        "    raise NotImplementedError('value_iteration')\n",
        "\n",
        "def value(q, s):\n",
        "    # Your code here (COPY FROM HW9)\n",
        "    raise NotImplementedError('value')\n",
        "\n",
        "def greedy(q, s):\n",
        "    # Your code here (COPY FROM HW9)\n",
        "    raise NotImplementedError('greedy')\n",
        "\n",
        "def epsilon_greedy(q, s, eps = 0.5):\n",
        "    if random.random() < eps:  # True with prob eps, random action\n",
        "        # Your code here (COPY FROM HW9)\n",
        "        raise NotImplementedError('epsilon_greedy')\n",
        "    else:\n",
        "        # Your code here (COPY FROM HW9)\n",
        "        raise NotImplementedError('epsilon_greedy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyKQPeWk5zx1"
      },
      "source": [
        "Run the next code block to make sure what you need from HW9 is working."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fvz2c_Vs3JuN"
      },
      "outputs": [],
      "source": [
        "mdp.value = value\n",
        "mdp.greedy = greedy\n",
        "mdp.epsilon_greedy = epsilon_greedy\n",
        "mdp.value_iteration = value_iteration\n",
        "\n",
        "importlib.reload(code_for_hw10)\n",
        "\n",
        "# Test: Value Iteration\n",
        "code_for_hw10.test_solve_play()\n",
        "\n",
        "# Expected output:\n",
        "# '''\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCaiqNqB6D2-"
      },
      "source": [
        "## 2.1) Q update\n",
        "\n",
        "First, we'll extend our implementation of the TabularQ class in HW 9 (Problem 5) to incorporate the crucial operation of Q-learning, which is to update the Q value for a given `(s, a)` entry and move it part of the way towards a \"target\" value *t*.\n",
        "\n",
        "> *Q(s,a) ← (1−α)Q(s,a) + αt*\n",
        "\n",
        "Note that this can also be written as:\n",
        "\n",
        "> *Q(s,a)← Q(s,a)+α(t−Q(s,a))*\n",
        "\n",
        "That is, move a small (*α*) step towards t."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL0934247JNK"
      },
      "source": [
        "Let's define a new method for `TabularQ` that implements this, in a batched form. We will be given a list of `(s, a, t)` triples and have to do all the updates. Note that update is a method of the `TabularQ` class, so you can access the other methods and attributes.\n",
        "\n",
        "* `data` is a list of `(s, a, t)` tuples.\n",
        "* `lr` is a learning rate (*α* above)\n",
        "* We will have to update `self.q[(s,a)]` for all of the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FZlzaFevNkD"
      },
      "outputs": [],
      "source": [
        "class TabularQ:\n",
        "    def __init__(self, states, actions):\n",
        "        self.actions = actions\n",
        "        self.states = states\n",
        "        self.q = dict([((s, a), 0.0) for s in states for a in actions])\n",
        "    def copy(self):\n",
        "        q_copy = TabularQ(self.states, self.actions)\n",
        "        q_copy.q.update(self.q)\n",
        "        return q_copy\n",
        "    def set(self, s, a, v):\n",
        "        self.q[(s,a)] = v\n",
        "    def get(self, s, a):\n",
        "        return self.q[(s,a)]\n",
        "    def update(self, data, lr):\n",
        "        # Your code here\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeQekM6h7vuL"
      },
      "source": [
        "## 2.2) Q_learn\n",
        "\n",
        "Complete the definition of the `Q_learn` function. It should update the entries in the `q` function, `(s, a)`, towards their estimated Q values. It should terminate after `iters` iterations and use learning rate `lr`. Use the `q.update` method, which you just wrote, to update the Q values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzaOD9VY7_4-"
      },
      "source": [
        "You will need to both simulate the agent's trajectory through the space as well as perform the updates to the Q function estimates. In this version, you should update the Q values after every transition, using a single `(s, a, t)` tuple. **The following methods and functions have already been defined for you.**\n",
        "\n",
        "\n",
        "* To start a new simulation, call `mdp.init_state()`. That will draw a state from the MDP's initial state distribution.\n",
        "* You can use the functions that we defined in HW 9: `epsilon_greedy` for action selection (epsilon_greedy takes `(q, s, eps = 0.5)` as input and returns an action) `value` takes `(q, s)` and returns the max Q value for a state.\n",
        "* To take a step in the simulation, starting in a given state, `s`, using action a, call `mdp.sim_transition(s,a)`. It will return a pair `(r, s_prime)` denoting the reward received by the agent at that step and the next state.\n",
        "* Be careful in treating terminal states. Recall that at a terminal state, there may be an immediate reward but the future expected value will be zero.\n",
        "* Return `q` so that the Tutor can test it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00YIgjEwvOab"
      },
      "outputs": [],
      "source": [
        "def Q_learn(mdp, q, lr=.1, iters=100, eps = 0.5, interactive_fn=None):\n",
        "    # Your code here\n",
        "    for i in range(iters):\n",
        "        # Your code here\n",
        "        # include this line in the iteration, where i is the iteration number\n",
        "        if interactive_fn: interactive_fn(q, i)\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UweE9iUL897r"
      },
      "source": [
        "Run the next code blocks to test your implementation of `Q_learn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rku6GFpdgMdH"
      },
      "outputs": [],
      "source": [
        "mdp.TabularQ = TabularQ\n",
        "mdp.Q_learn = Q_learn\n",
        "importlib.reload(code_for_hw10)\n",
        "\n",
        "# Test: Tabular Q-learn\n",
        "code_for_hw10.test_learn_play(iters=100000, tabular=True, batch=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzEz4q7y3R8a"
      },
      "outputs": [],
      "source": [
        "def tinyTerminal(s):\n",
        "    return s==4\n",
        "def tinyR(s, a):\n",
        "    if s == 1: return 1\n",
        "    elif s == 3: return 2\n",
        "    else: return 0\n",
        "def tinyTrans(s, a):\n",
        "    if s == 0:\n",
        "        if a == 'a':\n",
        "            return DDist({1 : 0.9, 2 : 0.1})\n",
        "        else:\n",
        "            return DDist({1 : 0.1, 2 : 0.9})\n",
        "    elif s == 1:\n",
        "        return DDist({1 : 0.1, 0 : 0.9})\n",
        "    elif s == 2:\n",
        "        return DDist({2 : 0.1, 3 : 0.9})\n",
        "    elif s == 3:\n",
        "        return DDist({3 : 0.1, 0 : 0.5, 4 : 0.4})\n",
        "    elif s == 4:\n",
        "        return DDist({4 : 1.0})\n",
        "\n",
        "def testQ():\n",
        "    tiny = mdp.MDP([0, 1, 2, 3, 4], ['a', 'b'], tinyTrans, tinyR, 0.9)\n",
        "    tiny.terminal = tinyTerminal\n",
        "    q = TabularQ(tiny.states, tiny.actions)\n",
        "    qf = Q_learn(tiny, q)\n",
        "    ret = list(qf.q.items())\n",
        "    expected = [((0, 'a'), 0.6649739221724159), ((0, 'b'), 0.1712369526453748),\n",
        "                ((1, 'a'), 0.7732751316011999), ((1, 'b'), 1.2034912054227331),\n",
        "                ((2, 'a'), 0.37197205380133874), ((2, 'b'), 0.45929063274463033),\n",
        "                ((3, 'a'), 1.5156163024818292), ((3, 'b'), 0.8776852768653631),\n",
        "                ((4, 'a'), 0.0), ((4, 'b'), 0.0)]\n",
        "    ok = True\n",
        "    for (s,a), v in expected:\n",
        "      qv = qf.get(s,a)\n",
        "      if abs(qv-v) > 1.0e-5:\n",
        "        print(\"Oops!  For (s=%s, a=%s) expected %s, but got %s\" % (s, a, v, qv))\n",
        "        ok = False\n",
        "    if ok:\n",
        "      print(\"Tests passed!\")\n",
        "\n",
        "random.seed(0)\n",
        "testQ()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PXWcgn99NfK"
      },
      "source": [
        "## 2.3) Batch Q_learn\n",
        "\n",
        "Assume your previous update method has been defined.\n",
        "\n",
        "In the standard Q-learning algorithm, we make one epsilon-greedy transition based on the current Q estimate and then update the Q values. You can think of this as being like stochastic gradient descent. We can also define a version that is more like batch gradient descent, where we generate one or more \"episodes\" (sequences of transitions) using the current Q values and then update the Q values based on all the observed results. We can also keep around old transitions and use them (all or a random subset) in the update as well. **Note that as our Q value estimate evolves, the target Q value computed from a previously observed transition can change.**\n",
        "\n",
        "Implement this version of batch Q-learning that (a) generates some specifed number of episodes of a given length (see `sim_episode` below), (b) adds these to the experiences we have seen previously, and (c) updates the Q estimates based on **all the experience so far**. Return `q` so that the Tutor can test it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WMcWSfnE-_Y"
      },
      "outputs": [],
      "source": [
        "# evaluate this cell so you can use the definition in your code below\n",
        "\n",
        "def sim_episode(mdp, episode_length, policy, draw=False):\n",
        "    '''\n",
        "    Simulate an episode (sequence of transitions) of at most\n",
        "    episode_length, using policy function to select actions.  If we find\n",
        "    a terminal state, end the episode.  Return accumulated reward a list\n",
        "    of (s, a, r, s') where s' is None for transition from terminal state.\n",
        "    Also return an animation if draw=True, or None if draw=False\n",
        "    '''\n",
        "    episode = []\n",
        "    reward = 0\n",
        "    s = mdp.init_state()\n",
        "    all_states = [s]\n",
        "    for i in range(episode_length):\n",
        "        a = policy(s)\n",
        "        (r, s_prime) = mdp.sim_transition(s, a)\n",
        "        reward += r\n",
        "        if mdp.terminal(s):\n",
        "            episode.append((s, a, r, None))\n",
        "            break\n",
        "        episode.append((s, a, r, s_prime))\n",
        "        if draw:\n",
        "            mdp.draw_state(s)\n",
        "        s = s_prime\n",
        "        all_states.append(s)\n",
        "    animation = animate(all_states, mdp.n, episode_length) if draw else None\n",
        "    return reward, episode, animation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IHoMJaI-ZhC"
      },
      "source": [
        "**Clarifications**\n",
        "\n",
        "* There should be a SINGLE call to `q.update` per iteration, not per episode or per experience. Just one call per iteration, with a lot of data.\n",
        "* Let's understand the distinction between experiences `(s, a, r, s')` and Q targets `(s, a, t)`. Note that experiences don't depend on the current estimated Q values (only on the environment we are acting in), but the \"t\" in the Q targets depends on the current Q values. So, it makes sense to store experiences across iterations, but not to store Q targets, since the Q targets change when we update our Q values. Thus, you want to continuously aggregate the experience and then, in each iteration, re-compute the Q targets under the current estimated Q values, then do the update with all of these Q targets. Here's pseudocode:\n",
        "\n",
        "\n",
        "```\n",
        "all_experiences = []\n",
        "Loop over n_iterations:\n",
        "    Loop over n_episodes:\n",
        "        Generate an episode of length episode_length, append this experience to all_experiences\n",
        "    all_q_targets = []\n",
        "    Loop over all_experiences:\n",
        "        Append Q target from one experience to all_q_targets\n",
        "        Remember to handle terminal states (where s' = None)\n",
        "    q.update(all_q_targets, lr)\n",
        "return q\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uHjoRWPvScc"
      },
      "outputs": [],
      "source": [
        "def Q_learn_batch(mdp, q, lr=.1, iters=100, eps=0.5,\n",
        "                  episode_length=10, n_episodes=2,\n",
        "                  interactive_fn=None):\n",
        "    # Your code here\n",
        "    for i in range(iters):\n",
        "        # Your code also here\n",
        "        # include this line in the iteration, where i is the iteration number\n",
        "        if interactive_fn: interactive_fn(q, i)\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv9oygbu9Sz1"
      },
      "source": [
        "Run the next code blocks to test your implementation of `Q_learn_batch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0FnwKY5gqNI"
      },
      "outputs": [],
      "source": [
        "mdp.Q_learn_batch = Q_learn_batch\n",
        "importlib.reload(code_for_hw10)\n",
        "\n",
        "# Test: Tabular Batch Q-learn\n",
        "code_for_hw10.test_learn_play(iters=10, tabular=True, batch=True) # Check: why do we want fewer iterations here?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFvMhZuL3-Y9"
      },
      "outputs": [],
      "source": [
        "def testBatchQ():\n",
        "    tiny = mdp.MDP([0, 1, 2, 3, 4], ['a', 'b'], tinyTrans, tinyR, 0.9)\n",
        "    tiny.terminal = tinyTerminal\n",
        "    q = TabularQ(tiny.states, tiny.actions)\n",
        "    qf = Q_learn_batch(tiny, q)\n",
        "    ret = list(qf.q.items())\n",
        "    expected = [((0, 'a'), 4.7566600197286535), ((0, 'b'), 3.993296047838986),\n",
        "                ((1, 'a'), 5.292467934685342), ((1, 'b'), 5.364014782870985),\n",
        "                ((2, 'a'), 4.139537149779127), ((2, 'b'), 4.155347555640753),\n",
        "                ((3, 'a'), 4.076532544818926), ((3, 'b'), 4.551442974149778),\n",
        "                ((4, 'a'), 0.0), ((4, 'b'), 0.0)]\n",
        "\n",
        "    ok = True\n",
        "    for (s,a), v in expected:\n",
        "      qv = qf.get(s,a)\n",
        "      if abs(qv-v) > 1.0e-5:\n",
        "        print(\"Oops!  For (s=%s, a=%s) expected %s, but got %s\" % (s, a, v, qv))\n",
        "        ok = False\n",
        "    if ok:\n",
        "      print(\"Tests passed!\")\n",
        "\n",
        "      return list(qf.q.items())\n",
        "\n",
        "random.seed(0)\n",
        "testBatchQ()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj5q0rup9eq0"
      },
      "source": [
        "# 3) NN Q: Using neural networks to store the Q function\n",
        "\n",
        "We would like to operate in large or continuous state and/or action\n",
        "spaces so it is not possible (or effective) to store the $Q$ values in\n",
        "a table as we did with the <tt>TabularQ</tt> class; instead, we will\n",
        "\"store\" them by training a neural network to do regression for us,\n",
        "taking $s,a$ as input and generating (an approximation of) $Q^*(s,a)$\n",
        "as output.\n",
        "\n",
        "To train the network, we will use <i>squared Bellman error</i> as\n",
        "the loss function:\n",
        "$$\\left(\\left[R(s_t, a_t) + \\gamma \\max_{a'} Q(s_{t+1}, a'; \\theta)\\right]\n",
        "- Q(s_t, a_t;\\theta) \\right)^2$$\n",
        "where $\\theta$ stands for the current weights in the neural network\n",
        "and $Q(s, a; \\theta)$ stands for the output of the network with\n",
        "weights $\\theta$ when $(s,a)$ is the input.\n",
        "\n",
        "There are many choices of neural network architecture for storing Q\n",
        "values.  In this problem, we will:\n",
        "\n",
        "<ul>\n",
        "\n",
        "<li> Focus on the case where we have a small set of possible actions,\n",
        "so make one neural network for each possible action <math>a</math>;\n",
        "\n",
        "<li> Design that network with two <b>hidden</b> layers with ReLU units\n",
        "and a single linear output unit (although a deeper network could be\n",
        "useful); and\n",
        "\n",
        "<li> Use mean squared error (MSE) as the loss function since, we are\n",
        "predicting continuous <math>Q</math> values, which is a regression\n",
        "problem.\n",
        "\n",
        "</ul>\n",
        "\n",
        "To use a neural net to store Q values, for a given action, we will\n",
        "need to have a mapping from states to fixed-length vectors.  We will\n",
        "assume that the <code>MDP</code> class has a <code>state2vec</code>\n",
        "method that maps states to vectors.  For the simple discrete-state\n",
        "MDPs we have seen so far, this simply returns a one-hot representation\n",
        "of the state.\n",
        "\n",
        "For reference, this is our implementation of\n",
        "<code>state2vec</code> (note the shape of its returned array):\n",
        "<pre>\n",
        "    def state2vec(self, s):\n",
        "        '''\n",
        "        Return one-hot encoding of state s; used in neural network agent implementations\n",
        "        '''\n",
        "        v = np.zeros((1, len(self.states)))\n",
        "        v[0,self.states.index(s)] = 1.\n",
        "        return v\n",
        "</pre>\n",
        "\n",
        "Now, all we need to do is write a new class, called <code>NNQ</code>\n",
        "to implement neural-network version of Q-function storage; then we can\n",
        "pass an <code>NNQ</code> instance instead of a <code>TabularQ</code> instance\n",
        "into <code>Q_learn</code> or <code>Q_learn_batch</code>, and\n",
        "we will automatically have reinforcement learning with neural\n",
        "networks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFEt8UXOR_v7"
      },
      "source": [
        "There are three methods to implement in our <code>NNQ</code>\n",
        "class. Here are some ideas for how to do that:\n",
        "<ul>\n",
        "\n",
        "<li> <code>__init__</code>: Create one neural network for each action,\n",
        "and store them in <tt>self.models</tt>. Note that <tt>actions</tt> is\n",
        "a list that may consist of integers or strings or other objects.  As a\n",
        "reminder, here's how to make a new feed-forward network using Keras:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqcuigrnLfY0"
      },
      "outputs": [],
      "source": [
        "# please evaluate this cell so you can use it in your code\n",
        "\n",
        "def make_nn(state_dim, num_hidden_layers, num_units):\n",
        "    '''\n",
        "    state_dim =\t(int) number of states\n",
        "    num_hidden_layers =\t(int) number of\tfully connected hidden layers\n",
        "    num_units =\t(int) number of\tdense relu units to use\tin hidden layers\n",
        "    '''\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_units, input_dim = state_dim, activation='relu'))\n",
        "    for i in range(num_hidden_layers-1):\n",
        "        model.add(Dense(num_units, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(loss='mse', optimizer=Adam())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozZOU_BnR4Tl"
      },
      "source": [
        "<ul>\n",
        "<li> <code>get(self, s, a)</code>: Use the neural network you have\n",
        "stored for action <code>a</code> to predict a Q value for state\n",
        "<code>s</code>. Feel free to consult documentation on the\n",
        "<a href=\"https://keras.io/models/model/#predict\">Keras model predict</a>\n",
        "method.</li>\n",
        "\n",
        "<li> <code>update(self, data, lr, epochs = 1)</code>: As in\n",
        "<code>TabularQ</code>, <code>data</code> is a list of <code>(s, a,t)</code>\n",
        "tuples, where <code>t</code> is a target Q value.  For each\n",
        "action <code>a</code>, you will need to:\n",
        "\n",
        "  <ul>\n",
        "  <li> Construct a training set <code>X, Y</code> of data that is\n",
        "  relevant to action <code>a</code>, where the input values are states\n",
        "  (encoded as vectors) and the output values are the target Q values;</li>\n",
        "\n",
        "  <li> Use the Keras method <code>fit(self, X, Y, epochs=epochs)</code>\n",
        "  to update the weights in th e associated network.  You can ignore\n",
        "  the <tt>lr</tt> input parameter, and let Adam in keras manage the\n",
        "  learning rate.</li>\n",
        "  </ul>\n",
        "</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKE6GBaxveRi"
      },
      "outputs": [],
      "source": [
        "# Complete the following definition to implement the NNQ class\n",
        "\n",
        "class NNQ:\n",
        "    def __init__(self, states, actions, state2vec, num_layers, num_units, epochs=1):\n",
        "        self.actions = actions\n",
        "        self.states = states\n",
        "        self.state2vec = state2vec\n",
        "        self.epochs = epochs\n",
        "        self.models = None              # Your code here\n",
        "    def get(self, s, a):\n",
        "        # Your code here\n",
        "        pass\n",
        "    def update(self, data, lr):\n",
        "        # Your code here\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ1PYVqe9Vq0"
      },
      "source": [
        "Run the next code blocks to test your implementation of `NNQ`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxQqOP7Qg2-S"
      },
      "outputs": [],
      "source": [
        "mdp.NNQ = NNQ\n",
        "importlib.reload(code_for_hw10)\n",
        "\n",
        "# Test: NN Q-learn\n",
        "code_for_hw10.test_learn_play(iters=100000, tabular=False, batch=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYNYWo7BvfIC"
      },
      "outputs": [],
      "source": [
        "def test_NNQ(data):\n",
        "    tiny = mdp.MDP([0, 1, 2, 3, 4], ['a', 'b'], tinyTrans, tinyR, 0.9)\n",
        "    tiny.terminal = tinyTerminal\n",
        "    q = NNQ(tiny.states, tiny.actions, tiny.state2vec, 2, 10)\n",
        "    q.update(data, 1)\n",
        "    ret =  [q.get(s,a) for s in q.states for a in q.actions]\n",
        "    expect = [np.array([[-0.07211456]]), np.array([[-0.19553234]]),\n",
        "              np.array([[-0.21926211]]), np.array([[0.01699455]]),\n",
        "              np.array([[-0.26390356]]), np.array([[0.06374809]]),\n",
        "              np.array([[0.0340214]]), np.array([[-0.18334733]]),\n",
        "              np.array([[-0.438375]]), np.array([[-0.13844737]])]\n",
        "    cnt = 0\n",
        "    ok = True\n",
        "    for s in q.states:\n",
        "      for a in q.actions:\n",
        "        if not np.all(np.abs(ret[cnt]-expect[cnt]) < 1.0e0):\n",
        "          print(\"Oops, for s=%s, a=%s expected %s but got %s\" % (s, a, expect[cnt], ret[cnt]))\n",
        "          ok = False\n",
        "        cnt += 1\n",
        "    if ok:\n",
        "      print(\"Output looks generally ok\")\n",
        "    return q\n",
        "\n",
        "test_NNQ([(0,'a',0.3),(1,'a',0.1),(0,'a',0.1),(1,'a',0.5)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quq-Uh44_Bbo"
      },
      "source": [
        "## 3.2) Fitted Q iteration\n",
        "\n",
        "*Fitted Q iteration (FQ)* suffers less from the correlated experience problem and is generally more stable (and sometimes slower) than NNQ.\n",
        "\n",
        "FQ initializes the Q networks and an empty data set, then operates in a loop:\n",
        "\n",
        "1. Use *ϵ*-greedy exploration to generate *k* steps of experience, of the form *(s,a,r,s′)* and add them to the data set.\n",
        "2. Create one training set for each action *a*:\n",
        "\n",
        "> 1. Extract all the tuples from your data set that contain action *a*,\n",
        "> 2. Let the *X* values of your training set be all of the *s* values from your data tuples with action *a* and the *Y* values be the *r + γ max_a' Q(s', a')* values computed for each data tuple, using the Q estimates from the current network.\n",
        "\n",
        "3. Train the network for action *a* for several epochs until it has done a good job of representing this data.\n",
        "\n",
        "So, this is basically `Q_learn_batch` using `NNQ` (training with multiple epochs) to implement the Q function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kW_URSm9mOI"
      },
      "source": [
        "Run the next code block to test your implementation of `NNQ` with batching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY6H72oV4z5O"
      },
      "outputs": [],
      "source": [
        "# Test: NN Batch Q-learn (Fitted Q-learn)\n",
        "code_for_hw10.test_learn_play(iters=10, tabular=False, batch=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d4QW3g0ww5H"
      },
      "source": [
        "#4) No Exit\n",
        "\n",
        "Please read the instructions in the [homework](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week10/week10_homework/) to set up for the game. You may use this space to display the game in Colab.\n",
        "\n",
        "For each of the learning method and Q model combinations below,\n",
        "solve the game so that it reliably gets to reward of 100 (that is, the\n",
        "learned game reliably plays 100 steps without missing the ball,\n",
        "earning a score of 100).  During learning, you should see a sequence\n",
        "of lines like: <code>score (5000, 37.5)</code>, which indicates that\n",
        "after 5000 iterations the average reward over 10 games is 37.5.  We\n",
        "are checking whether you reach a solution that gets an average reward\n",
        "100 at least one time. Try playing around with the number of\n",
        "iterations (an argument to <code>test_learn_play</code>) until you\n",
        "achieve this point. Note that we will need fewer iterations for\n",
        "Q_learn_batch, in general (check yourself: why?). After learning, the\n",
        "code prints a long \"upload string\" in HEX code.  Enter the upload\n",
        "strings in the question boxes in the homework MITx site."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRbeO95qxJf4"
      },
      "outputs": [],
      "source": [
        "# Value Iteration\n",
        "code_for_hw10.test_solve_play(draw = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXA-F66nWizC"
      },
      "outputs": [],
      "source": [
        "# Tabular Q-learn\n",
        "code_for_hw10.test_learn_play(draw=True, iters=100000, tabular=True, batch=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xWEvfLiWmdY"
      },
      "outputs": [],
      "source": [
        "# Tabular Batch Q-learn\n",
        "code_for_hw10.test_learn_play(draw=True, iters=10, tabular=True, batch=True) # Check: why do we want fewer iterations here?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtiEnPOZWrNa"
      },
      "outputs": [],
      "source": [
        "# NN Q-learn\n",
        "code_for_hw10.test_learn_play(draw=True, iters=100000, tabular=False, batch=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqGjYEq5WuKv"
      },
      "outputs": [],
      "source": [
        "# NN Batch Q-learn (Fitted Q-learn)\n",
        "code_for_hw10.test_learn_play(draw=True, iters=10, tabular=False, batch=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
